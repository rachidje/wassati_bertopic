{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14080/112462886.py:1: DtypeWarning: Columns (5,6,14,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_old = pd.read_csv(\"../data/csv_files/schneider_processed_labelled_full.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_old = pd.read_csv(\"../data/csv_files/schneider_processed_labelled_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.schneider import countries_to_update, text_data_column, words_to_filter, replacements\n",
    "from preprocessing.dataLoaders.schneider_data_loader import SchneiderDataLoader\n",
    "from preprocessing.preprocessing import Preprocessor\n",
    "\n",
    "df = pd.read_csv(\"../data/csv_files/schneider_2023.csv\")\n",
    "schneiderDataLoader = SchneiderDataLoader(df, countries_to_update)\n",
    "preprocessing = Preprocessor(\n",
    "    schneiderDataLoader, \n",
    "    text_data_column,\n",
    "    words_to_filter,\n",
    "    replacements\n",
    ")\n",
    "\n",
    "df_2023_preprocessed_full = preprocessing.preprocess()\n",
    "# Filter rows based on 'non_empty_rows' column\n",
    "df_2023_preprocessed = df_2023_preprocessed_full[df_2023_preprocessed_full['non_empty_rows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/vectorizers/_ctfidf.py:69: RuntimeWarning: divide by zero encountered in divide\n",
      "  idf = np.log((avg_nr_samples / df)+1)\n"
     ]
    }
   ],
   "source": [
    "def load_bertopic_model(filename):\n",
    "    \"\"\"\n",
    "    Load a BERTopic model and associated data from a file.\n",
    "    \n",
    "    :param filename: The name of the file to load the data from.\n",
    "    :return: A tuple containing the loaded BERTopic model, topics, probs, and docs variables.\n",
    "    \"\"\"\n",
    "    # Load the BERTopic model\n",
    "    topic_model = BERTopic.load(filename)\n",
    "    \n",
    "    # Load the topics, probs, and docs variables\n",
    "    with open(filename + '_data.pkl', 'rb') as f:\n",
    "        topics, probs, embeddings, docs = pickle.load(f)\n",
    "    \n",
    "    return topic_model, topics, probs, embeddings, docs\n",
    "\n",
    "topic_model, topics, probs, embeddings, docs = load_bertopic_model('../models/raw_keybert_bertopic_model')\n",
    "\n",
    "def create_merged_model(docs, bertopic_model, topics_to_merge_dict, label_names_dict):\n",
    "    \"\"\"\n",
    "    Create a new BERTopic model by merging topics from an existing model.\n",
    "\n",
    "    This function takes as input a list of documents `docs`, an existing BERTopic model `bertopic_model`, a dictionary `topics_to_merge_dict` specifying which topics to merge, and a dictionary `label_names_dict` specifying the labels for the merged topics.\n",
    "\n",
    "    The function creates a deep copy of the input BERTopic model and merges the specified topics using the `merge_topics` method. Then, it sets the topic labels for the merged model using the `set_topic_labels` method and the provided `label_names_dict`.\n",
    "\n",
    "    The resulting merged BERTopic model is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        bertopic_model (BERTopic): The input BERTopic model to be merged.\n",
    "        topics_to_merge_dict (dict): A dictionary specifying which topics to merge. The keys are the topic numbers to be merged, and the values are the topic numbers into which they should be merged.\n",
    "        label_names_dict (dict): A dictionary specifying the labels for the merged topics. The keys are the topic numbers, and the values are the corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        BERTopic: The resulting merged BERTopic model.\n",
    "    \"\"\"\n",
    "    topic_model_merged = copy.deepcopy(bertopic_model)\n",
    "    topic_model_merged.merge_topics(docs, topics_to_merge_dict)\n",
    "\n",
    "    # Create a dictionary to match the aggregated name to their corresponding topic number\n",
    "    mergedtopic_labels_dict = {i-1: item for i, item in enumerate(label_names_dict)}\n",
    "    # Set topic labels for the aggregated model\n",
    "    topic_model_merged.set_topic_labels(mergedtopic_labels_dict)\n",
    "\n",
    "    return topic_model_merged### Création du modèle bertopic aggrégé pour topics finaux\n",
    "\n",
    "# List of topics numbers. Each value of this list is a list that contains the topic number of the topics to join together\n",
    "topics_to_merge = [ [42,3,0,13], #Delivery Deadlines : challenges and strategies involved in managing delivery deadlines in logistics operations. (vert)\n",
    "                    [20,50,27], #Quotation and Pricing Strategies (vert bas)\n",
    "                    [35,32], #Touch Panels and Screens (rouge, haut)\n",
    "                    [40,36], #Frequency Converters : frequency converters used in industrial applications and the technical support provided by manufacturers and suppliers (rouge, suite)\n",
    "                    [37,21,6,12,9,4,1,14,16,31,19], #“Automation Components” : hardware and software components used in industrial automation systems. (rouge centre)\n",
    "                    [33,46,8], #Product Evaluation : evaluate the quality, affordability and reliability of products and services (rouge, fin)\n",
    "                    [44,51,23,41,49,57,22], #Customer Support : Reliability and Quality in Customer Service and Support (bleu ciel)\n",
    "                    [58,59], #Quick Customer Service (marron)\n",
    "                    [38,10,26,52,39,43], #Problem Solving and Communication (focus on the importance of being efficient and precise when solving problems) (jaune)\n",
    "                    [45,47,55,53,54], #Assistance and Guidance (noir)\n",
    "                    [29,30,11,24], #Power Supply Issues (2e vert, haut)\n",
    "                    [7,5,2,25,15,34,18,28,17], #Technical Support (2e vert, bas)\n",
    "                    [48,56] #None : positive feedback (2e rouge)\n",
    "]\n",
    "\n",
    "label_names_postmerge = [\n",
    "    \"Outliers\",\n",
    "    \"Automation Components\",\n",
    "    \"Technical Support\",\n",
    "    \"Delivery Deadlines\",\n",
    "    \"Problem Solving & Comm\",\n",
    "    \"Power Supply Issues\",\n",
    "    \"Customer Support\", #Reliability and Quality in Customer Service and Support\n",
    "    \"Product Evaluation\",\n",
    "    \"Pricing\", #Quotation and Pricing Strategies\n",
    "    \"Assistance\", #Assistance and Guidance\n",
    "    \"Touch Screens\", #Touch Panels and Screens\n",
    "    \"Frequency Converters\",\n",
    "    \"Positive feedback\",\n",
    "    \"Quick Customer Service\"\n",
    "    ]\n",
    "\n",
    "# Set the topic names for the new aggregated topic\n",
    "# It must match the order from the topics_to_merge list\n",
    "label_names = [\"Delivery Deadlines\",\n",
    "    \"Pricing\", #Quotation and Pricing Strategies\n",
    "    \"Touch Screens\", #Touch Panels and Screens\n",
    "    \"Frequency Converters\",\n",
    "    \"Automation Components\",\n",
    "    \"Product Evaluation\",\n",
    "    \"Customer Support\", #Reliability and Quality in Customer Service and Support\n",
    "    \"Quick Customer Service\",\n",
    "    \"Problem Solving & Comm\",\n",
    "    \"Assistance\", #Assistance and Guidance\n",
    "    \"Power Supply Issues\",\n",
    "    \"Technical Support\",\n",
    "    \"positive feedback\"]\n",
    "\n",
    "topic_labels_dict = {}\n",
    "topic_labels_dict[-1]=\"Outliers\"\n",
    "for i in range(len(topics_to_merge)):\n",
    "    for topic in topics_to_merge[i]:\n",
    "        topic_labels_dict[topic] = label_names[i]\n",
    "\n",
    "# Create a new merged bertopic model \n",
    "topic_model_merged = create_merged_model(docs, topic_model, topics_to_merge, label_names_postmerge)\n",
    "\n",
    "# topic_model_merged = copy.deepcopy(topic_model)\n",
    "# topic_model_merged.set_topic_labels(topic_labels_dict)\n",
    "# topic_model_merged.merge_topics(docs, topics_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_preprocessed.reset_index(drop=True, inplace=True)\n",
    "new_topics, new_probs = topic_model_merged.transform(df_2023_preprocessed['processed_data'])\n",
    "\n",
    "# Now, you can merge the new dataframe with the old one\n",
    "# df_labelled = pd.concat([df_labelled, df_2023_preprocessed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic and keywords for the new data\n",
    "new_topic_keywords = {}\n",
    "n = 10  # Number of keywords to include\n",
    "for topic in topic_model_merged.get_topics().keys():\n",
    "    topic_words = [word for word, _ in topic_model_merged.get_topic(topic)[:n]]\n",
    "    new_topic_keywords[topic] = \", \".join(topic_words)\n",
    "\n",
    "# Add the topics and keywords to the new dataframe\n",
    "df_2023_preprocessed['topic'] = new_topics\n",
    "df_2023_preprocessed['keywords'] = df_2023_preprocessed['topic'].map(new_topic_keywords)\n",
    "\n",
    "# Add the label column to the new dataframe\n",
    "mergedtopic_labels_dict = {i-1: item for i, item in enumerate(label_names_postmerge)}\n",
    "df_2023_preprocessed['label'] = df_2023_preprocessed['topic'].map(mergedtopic_labels_dict)\n",
    "\n",
    "# Get a list of the topics in the correct order, excluding topic -1\n",
    "topic_order = [topic for topic in topic_model_merged.get_topics().keys() if topic != -1]\n",
    "\n",
    "# Create a dictionary where the keys are the topic labels and the values are the probabilities\n",
    "new_proba_dict = []\n",
    "for proba in new_probs:\n",
    "    new_proba_dict.append(dict(sorted(zip(topic_order, proba), key=lambda item: item[1], reverse=True)))\n",
    "    \n",
    "# Add the proba_dict column to the new dataframe\n",
    "df_2023_preprocessed['proba_dict'] = new_proba_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can append the two dataframes, the 2023 data not null with the null rows from 2023_full\n",
    "df_final = df_2023_preprocessed.append(df_2023_preprocessed_full[df_2023_preprocessed_full['non_empty_rows'] == False], ignore_index=True)\n",
    "df_final['Response Date'] = pd.to_datetime(df_final['Response Date'])\n",
    "df_final['year_month'] = df_final['Response Date'].dt.to_period('M').dt.to_timestamp()\n",
    "df_final['Market Segment'] = df_final['Market Segment'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survey Type', 'Year'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_final.columns) - set(df_old.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "### Analyse de Sentiments simple : positif - négatif - neutre\n",
    "\n",
    "def load_model_huggingface(model_name, task, problem_type=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This function loads a model and tokenizer from a given model name, then creates a pipeline to perform a specified task.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "        task (str): The type of task to perform with the pipeline.\n",
    "        problem_type (str): The type of problem to solve (\"multi_label_classification\" for multi-label tasks).\n",
    "        **kwargs: Additional arguments to pass to the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        pipeline: A pipeline configured to perform the specified task with the loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=problem_type)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    classifier = pipeline(task, model=model, tokenizer=tokenizer, **kwargs)\n",
    "    return classifier\n",
    "\n",
    "def add_single_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function merges the DataFrame of single-label predictions with the original DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a dictionary containing a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Convert the predictions to a DataFrame\n",
    "    prediction_results = pd.DataFrame(predictions)\n",
    "    prediction_results.rename(columns={'label': predicted_column_name}, inplace=True)\n",
    "    # # Reset the indices of the DataFrames (if necessary)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # prediction_results.reset_index(drop=True, inplace=True)\n",
    "    # Merge the original DataFrame with the prediction results\n",
    "    df_predicted = pd.concat([predicted_df, prediction_results], axis=1)\n",
    "    return df_predicted\n",
    "\n",
    "def add_multi_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function adds a new column with multi-label predictions to the DataFrame and also adds two more columns for \n",
    "    the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a list of dictionaries, where each dictionary \n",
    "                            contains a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores, as well as \n",
    "                      columns for the best label and its score.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Keep the original predictions as they are (a list of dictionaries) and add them to the DataFrame as a new column\n",
    "    predicted_df[predicted_column_name] = predictions\n",
    "    # Add columns for the best label and its score\n",
    "    predicted_df[f'best_{predicted_column_name}'] = predicted_df[predicted_column_name].apply(lambda x: max(x.keys(), key=lambda k: x[k]) if x else None)\n",
    "    predicted_df[f'best_{predicted_column_name}_score'] = predicted_df[predicted_column_name].apply(lambda x: x[max(x.keys(), key=lambda k: x[k])] if x else None)\n",
    "    return predicted_df\n",
    "\n",
    "def make_predictions_df(classifier, df, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function makes predictions on a DataFrame of documents using a given classifier. It adds the predictions to \n",
    "    the DataFrame as new columns. If the classifier is for single-label classification, it adds one column for the \n",
    "    predicted label and one for the score. If the classifier is for multi-label classification, it adds one column \n",
    "    with a dictionary of label-score pairs for each document, and two additional columns for the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        classifier (pipeline): The Hugging Face pipeline object for making predictions.\n",
    "        df (pd.DataFrame): The DataFrame containing the documents to make predictions on. It must have a 'processed_data' \n",
    "                           column with the preprocessed text of each document.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame for the predictions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predictions.\n",
    "    \"\"\"\n",
    "    # Get the list of documents from the DataFrame\n",
    "    docs = df[\"processed_data\"].tolist()\n",
    "    # Get predictions\n",
    "    predictions = classifier(docs)\n",
    "    \n",
    "    # Check if predictions is a list of dictionaries (single-label case)\n",
    "    if isinstance(predictions, list) and isinstance(predictions[0], dict):\n",
    "        df_predicted = add_single_label_predictions(df, predictions, predicted_column_name)\n",
    "    \n",
    "    # Multi-label case\n",
    "    elif isinstance(predictions, list) and isinstance(predictions[0], list):\n",
    "        df_predicted = add_multi_label_predictions(df, predictions, predicted_column_name)\n",
    "\n",
    "    return df_predicted\n",
    "\n",
    "classifier = load_model_huggingface(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text-classification\", max_length=512, truncation=True)\n",
    "predicted_df = make_predictions_df(classifier, df_final, 'sentiment_label')\n",
    "\n",
    "predictions = []\n",
    "for review in [[x] for x in df_final.processed_data.tolist()]:\n",
    "      # Effectuer une prédiction pour le document en utilisant le classificateur\n",
    "    prediction = classifier(review)\n",
    "      # Ajouter la prédiction à la liste des prédictions\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Création d'un DataFrame à partir des résultats de prédiction\n",
    "res = pd.DataFrame([item for sublist in predictions for item in sublist])\n",
    "df_pred = pd.concat([df_final, res], axis=1)\n",
    "\n",
    "# The model created a column 'label' to store the prediction, BUT the df already had one column 'label', then :\n",
    "# Rename the second 'label' column to 'sentiment_label'\n",
    "cols = df_pred.columns.tolist()\n",
    "cols[len(cols) - 1 - cols[::-1].index('label')] = 'sentiment_label'\n",
    "df_pred.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ast\n",
    "\n",
    "### Analyse de Sentiments approfondie : par 28 émotions\n",
    "\n",
    "# en multilabel, avec GPU et par batch pour accélerer le traitement\n",
    "\n",
    "# Check if a CUDA-enabled GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=\"multi_label_classification\", max_length=512)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create a list of label names\n",
    "label_emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise','neutral']\n",
    "\n",
    "# Initialize lists to store the predicted labels and scores\n",
    "predicted_labels = []\n",
    "predicted_scores = []\n",
    "\n",
    "df_emotion = df_pred\n",
    "# Iterate over the rows of the DataFrame in batches\n",
    "for i in range(0, len(df_emotion), batch_size):\n",
    "    batch = df_emotion[i:i+batch_size]\n",
    "    texts = batch['processed_data'].tolist()\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move the inputs to the GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits.sigmoid().detach().cpu().numpy()\n",
    "    \n",
    "    # Apply a threshold to the probabilities to get the predicted labels\n",
    "    threshold = 0.5\n",
    "    labels = [[label_emotions[i] for i, prob in enumerate(prob_row) if prob > threshold] for prob_row in probs]\n",
    "    \n",
    "    # Store the predicted labels and scores\n",
    "    predicted_labels.extend(labels)\n",
    "    scores = [{label_emotions[i]: prob for i, prob in enumerate(prob_row)} for prob_row in probs]\n",
    "    # predicted_scores.extend(probs.tolist())\n",
    "    predicted_scores.extend(scores)\n",
    "\n",
    "# Add the predicted labels and scores as new columns in the DataFrame\n",
    "df_emotion['predicted_labels'] = predicted_labels\n",
    "df_emotion['predicted_scores'] = predicted_scores\n",
    "\n",
    "# # TO DO when reading the df, since the list columns will be read as full string and not list...\n",
    "# # Convert the string values in the 'predicted_labels' column into lists\n",
    "# df_emotion['predicted_labels'] = df_emotion['predicted_labels'].apply(lambda x: ast.literal_eval(x))\n",
    "# # Convert the string values in the 'predicted_scores' column into dictionaries\n",
    "# df_emotion['predicted_scores'] = df_emotion['predicted_scores'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict(input_dict, conversion_type='keys_to_values'):\n",
    "    \"\"\"\n",
    "    This function converts between a key-to-value dictionary and a value-to-keys dictionary.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): The input dictionary. If conversion_type is 'keys_to_values', this should be a dictionary where \n",
    "                           the keys are the original keys and the values are the corresponding values. If conversion_type is \n",
    "                           'values_to_keys', this should be a dictionary where the keys are the original values and the values \n",
    "                           are lists of keys for each value.\n",
    "        conversion_type (str): The type of conversion to perform. Can be either 'keys_to_values' or 'values_to_keys'.\n",
    "\n",
    "    Returns:\n",
    "        dict: The converted dictionary. If conversion_type is 'keys_to_values', this will be a dictionary where the keys \n",
    "              are the original values and the values are lists of keys for each value. If conversion_type is 'values_to_keys', \n",
    "              this will be a dictionary where the keys are the original keys and the values are the corresponding values.\n",
    "    \"\"\"\n",
    "    if conversion_type == 'keys_to_values':\n",
    "        output_dict = {}\n",
    "        for key, value in input_dict.items():\n",
    "            if value not in output_dict:\n",
    "                output_dict[value] = []\n",
    "            output_dict[value].append(key)\n",
    "    elif conversion_type == 'values_to_keys':\n",
    "        output_dict = {key: value for value, keys in input_dict.items() for key in keys}\n",
    "    else:\n",
    "        raise ValueError(\"Invalid conversion_type. Must be either 'keys_to_values' or 'values_to_keys'.\")\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def add_grouping_column(df, key_column, group_dict, group_column_name):\n",
    "    \"\"\"\n",
    "    This function adds a new column to a DataFrame with the group name of each key.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        key_column (str): The name of the column in df that contains the keys.\n",
    "        group_dict (dict): A dictionary where the keys are the original keys and the values are the corresponding group names.\n",
    "        group_column_name (str): The name of the new column to be added to df for the groups.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an added column for the groups.\n",
    "    \"\"\"\n",
    "    # df[group_column_name] = df[key_column].map(group_dict)\n",
    "    df[group_column_name] = df[key_column].apply(lambda labels: [group_dict[label] for label in labels if label in group_dict])\n",
    "    return df\n",
    "\n",
    "# transform the multilabel results from the emotions analysis in single label\n",
    "# Define the groups\n",
    "positive_emotions = ['admiration','approval','gratitude','caring','realization','joy','optimism','love','excitement','amusement','relief']\n",
    "negative_emotions = ['disappointment','disapproval','annoyance','confusion','nervousness','fear','sadness','remorse','disgust','embarrassment','anger']\n",
    "neutral_emotions = ['neutral','desire','surprise','curiosity']\n",
    "# Create a dictionary where the keys are the group names and the values are the lists of labels\n",
    "groups = {\"positive\": positive_emotions, \"negative\": negative_emotions, \"neutral\": neutral_emotions}\n",
    "# Convert the groups to a group_dict\n",
    "group_dict = convert_dict(groups, conversion_type='values_to_keys')\n",
    "\n",
    "# Now you can use group_dict with add_grouping_column\n",
    "# df_emotion = add_grouping_column(df_emotion, \"predicted_labels\", group_dict, \"sentiment_from_emotion_label\")\n",
    "\n",
    "# Create a new column 'single_emotion_label' that contains the label with the highest score\n",
    "df_emotion['single_emotion_label'] = df_emotion['predicted_scores'].apply(lambda x: max(x, key=x.get))\n",
    "# Create a new column 'sentiment' that contains the sentiment of the emotion in the 'single_emotion_label' column\n",
    "df_emotion['single_sentiment_from_emotion'] = df_emotion['single_emotion_label'].apply(lambda x: 'positive' if x in positive_emotions else ('negative' if x in negative_emotions else ('neutral' if x in neutral_emotions else 'unknown')))\n",
    "\n",
    "# df3.to_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_labels</th>\n",
       "      <th>predicted_scores</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>keywords</th>\n",
       "      <th>single_emotion_label</th>\n",
       "      <th>single_sentiment_from_emotion</th>\n",
       "      <th>sentiment_from_emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>{'admiration': 0.001663754, 'amusement': 0.000...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Outliers</td>\n",
       "      <td>customer service, delivery time, sales, techni...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>{'admiration': 0.104325175, 'amusement': 0.000...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Outliers</td>\n",
       "      <td>customer service, delivery time, sales, techni...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>{'admiration': 0.0010179136, 'amusement': 0.00...</td>\n",
       "      <td>negative</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Delivery Deadlines</td>\n",
       "      <td>delivery time, delays, delivery date, deliveri...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[admiration]</td>\n",
       "      <td>{'admiration': 0.84221095, 'amusement': 0.0011...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>technical support, response time, customer ser...</td>\n",
       "      <td>admiration</td>\n",
       "      <td>positive</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>{'admiration': 0.0072880113, 'amusement': 0.00...</td>\n",
       "      <td>positive</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pricing</td>\n",
       "      <td>pricing, prices, discount, quotation, orders, ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predicted_labels                                   predicted_scores  \\\n",
       "0        [neutral]  {'admiration': 0.001663754, 'amusement': 0.000...   \n",
       "1               []  {'admiration': 0.104325175, 'amusement': 0.000...   \n",
       "2        [neutral]  {'admiration': 0.0010179136, 'amusement': 0.00...   \n",
       "3     [admiration]  {'admiration': 0.84221095, 'amusement': 0.0011...   \n",
       "4        [neutral]  {'admiration': 0.0072880113, 'amusement': 0.00...   \n",
       "\n",
       "  sentiment_label  topic               label  \\\n",
       "0         neutral   -1.0            Outliers   \n",
       "1         neutral   -1.0            Outliers   \n",
       "2        negative    2.0  Delivery Deadlines   \n",
       "3        positive    1.0   Technical Support   \n",
       "4        positive    7.0             Pricing   \n",
       "\n",
       "                                            keywords single_emotion_label  \\\n",
       "0  customer service, delivery time, sales, techni...              neutral   \n",
       "1  customer service, delivery time, sales, techni...              neutral   \n",
       "2  delivery time, delays, delivery date, deliveri...              neutral   \n",
       "3  technical support, response time, customer ser...           admiration   \n",
       "4  pricing, prices, discount, quotation, orders, ...              neutral   \n",
       "\n",
       "  single_sentiment_from_emotion sentiment_from_emotion_label  \n",
       "0                       neutral                    [neutral]  \n",
       "1                       neutral                           []  \n",
       "2                       neutral                    [neutral]  \n",
       "3                      positive                   [positive]  \n",
       "4                       neutral                    [neutral]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df_emotion[[\"predicted_labels\", 'predicted_scores', 'sentiment_label', 'topic','label','keywords','single_emotion_label','single_sentiment_from_emotion','sentiment_from_emotion_label']].head()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62843"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion[\"allComment\"] = df_emotion.processed_data\n",
    "df_2023_full =df_emotion.append(df_old, ignore_index=True)\n",
    "len(df_2023_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion =  df_emotion[df_emotion.columns.difference(['Survey Type', 'Year'])]\n",
    "df_schneider_full = df_old.append(df_emotion, ignore_index=True)\n",
    "df_schneider_full.to_csv(\"../data/csv_files/schneider_all_processed_labelled_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wassati",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
