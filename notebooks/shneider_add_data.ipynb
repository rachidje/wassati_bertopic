{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5979/112462886.py:1: DtypeWarning:\n",
      "\n",
      "Columns (5,6,14,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_old = pd.read_csv(\"../data/csv_files/schneider_processed_labelled_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.schneider import countries_to_update, text_data_column, words_to_filter, replacements\n",
    "from preprocessing.dataLoaders.schneider_data_loader import SchneiderDataLoader\n",
    "from preprocessing.preprocessing import Preprocessor\n",
    "\n",
    "df = pd.read_csv(\"../data/csv_files/schneider_2023.csv\")\n",
    "schneiderDataLoader = SchneiderDataLoader(df, countries_to_update)\n",
    "preprocessing = Preprocessor(\n",
    "    schneiderDataLoader, \n",
    "    text_data_column,\n",
    "    words_to_filter,\n",
    "    replacements\n",
    ")\n",
    "\n",
    "df_2023_preprocessed_full = preprocessing.preprocess()\n",
    "# Filter rows based on 'non_empty_rows' column\n",
    "df_2023_preprocessed = df_2023_preprocessed_full[df_2023_preprocessed_full['non_empty_rows']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/vectorizers/_ctfidf.py:69: RuntimeWarning: divide by zero encountered in divide\n",
      "  idf = np.log((avg_nr_samples / df)+1)\n"
     ]
    }
   ],
   "source": [
    "def load_bertopic_model(filename):\n",
    "    \"\"\"\n",
    "    Load a BERTopic model and associated data from a file.\n",
    "    \n",
    "    :param filename: The name of the file to load the data from.\n",
    "    :return: A tuple containing the loaded BERTopic model, topics, probs, and docs variables.\n",
    "    \"\"\"\n",
    "    # Load the BERTopic model\n",
    "    topic_model = BERTopic.load(filename)\n",
    "    \n",
    "    # Load the topics, probs, and docs variables\n",
    "    with open(filename + '_data.pkl', 'rb') as f:\n",
    "        topics, probs, embeddings, docs = pickle.load(f)\n",
    "    \n",
    "    return topic_model, topics, probs, embeddings, docs\n",
    "\n",
    "topic_model, topics, probs, embeddings, docs = load_bertopic_model('../models/raw_keybert_bertopic_model')\n",
    "\n",
    "def create_merged_model(docs, bertopic_model, topics_to_merge_dict, label_names_dict):\n",
    "    \"\"\"\n",
    "    Create a new BERTopic model by merging topics from an existing model.\n",
    "\n",
    "    This function takes as input a list of documents `docs`, an existing BERTopic model `bertopic_model`, a dictionary `topics_to_merge_dict` specifying which topics to merge, and a dictionary `label_names_dict` specifying the labels for the merged topics.\n",
    "\n",
    "    The function creates a deep copy of the input BERTopic model and merges the specified topics using the `merge_topics` method. Then, it sets the topic labels for the merged model using the `set_topic_labels` method and the provided `label_names_dict`.\n",
    "\n",
    "    The resulting merged BERTopic model is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        bertopic_model (BERTopic): The input BERTopic model to be merged.\n",
    "        topics_to_merge_dict (dict): A dictionary specifying which topics to merge. The keys are the topic numbers to be merged, and the values are the topic numbers into which they should be merged.\n",
    "        label_names_dict (dict): A dictionary specifying the labels for the merged topics. The keys are the topic numbers, and the values are the corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        BERTopic: The resulting merged BERTopic model.\n",
    "    \"\"\"\n",
    "    topic_model_merged = copy.deepcopy(bertopic_model)\n",
    "    topic_model_merged.merge_topics(docs, topics_to_merge_dict)\n",
    "\n",
    "    # Create a dictionary to match the aggregated name to their corresponding topic number\n",
    "    mergedtopic_labels_dict = {i-1: item for i, item in enumerate(label_names_dict)}\n",
    "    # Set topic labels for the aggregated model\n",
    "    topic_model_merged.set_topic_labels(mergedtopic_labels_dict)\n",
    "\n",
    "    return topic_model_merged### Création du modèle bertopic aggrégé pour topics finaux\n",
    "\n",
    "# List of topics numbers. Each value of this list is a list that contains the topic number of the topics to join together\n",
    "topics_to_merge = [ [42,3,0,13], #Delivery Deadlines : challenges and strategies involved in managing delivery deadlines in logistics operations. (vert)\n",
    "                    [20,50,27], #Quotation and Pricing Strategies (vert bas)\n",
    "                    [35,32], #Touch Panels and Screens (rouge, haut)\n",
    "                    [40,36], #Frequency Converters : frequency converters used in industrial applications and the technical support provided by manufacturers and suppliers (rouge, suite)\n",
    "                    [37,21,6,12,9,4,1,14,16,31,19], #“Automation Components” : hardware and software components used in industrial automation systems. (rouge centre)\n",
    "                    [33,46,8], #Product Evaluation : evaluate the quality, affordability and reliability of products and services (rouge, fin)\n",
    "                    [44,51,23,41,49,57,22], #Customer Support : Reliability and Quality in Customer Service and Support (bleu ciel)\n",
    "                    [58,59], #Quick Customer Service (marron)\n",
    "                    [38,10,26,52,39,43], #Problem Solving and Communication (focus on the importance of being efficient and precise when solving problems) (jaune)\n",
    "                    [45,47,55,53,54], #Assistance and Guidance (noir)\n",
    "                    [29,30,11,24], #Power Supply Issues (2e vert, haut)\n",
    "                    [7,5,2,25,15,34,18,28,17], #Technical Support (2e vert, bas)\n",
    "                    [48,56] #None : positive feedback (2e rouge)\n",
    "]\n",
    "\n",
    "label_names_postmerge = [\n",
    "    \"Outliers\",\n",
    "    \"Automation Components\",\n",
    "    \"Technical Support\",\n",
    "    \"Delivery Deadlines\",\n",
    "    \"Problem Solving & Comm\",\n",
    "    \"Power Supply Issues\",\n",
    "    \"Customer Support\", #Reliability and Quality in Customer Service and Support\n",
    "    \"Product Evaluation\",\n",
    "    \"Pricing\", #Quotation and Pricing Strategies\n",
    "    \"Assistance\", #Assistance and Guidance\n",
    "    \"Touch Screens\", #Touch Panels and Screens\n",
    "    \"Frequency Converters\",\n",
    "    \"Positive feedback\",\n",
    "    \"Quick Customer Service\"\n",
    "    ]\n",
    "\n",
    "# Set the topic names for the new aggregated topic\n",
    "# It must match the order from the topics_to_merge list\n",
    "label_names = [\"Delivery Deadlines\",\n",
    "    \"Pricing\", #Quotation and Pricing Strategies\n",
    "    \"Touch Screens\", #Touch Panels and Screens\n",
    "    \"Frequency Converters\",\n",
    "    \"Automation Components\",\n",
    "    \"Product Evaluation\",\n",
    "    \"Customer Support\", #Reliability and Quality in Customer Service and Support\n",
    "    \"Quick Customer Service\",\n",
    "    \"Problem Solving & Comm\",\n",
    "    \"Assistance\", #Assistance and Guidance\n",
    "    \"Power Supply Issues\",\n",
    "    \"Technical Support\",\n",
    "    \"positive feedback\"]\n",
    "\n",
    "topic_labels_dict = {}\n",
    "topic_labels_dict[-1]=\"Outliers\"\n",
    "for i in range(len(topics_to_merge)):\n",
    "    for topic in topics_to_merge[i]:\n",
    "        topic_labels_dict[topic] = label_names[i]\n",
    "\n",
    "# Create a new merged bertopic model \n",
    "topic_model_merged = create_merged_model(docs, topic_model, topics_to_merge, label_names_postmerge)\n",
    "\n",
    "# topic_model_merged = copy.deepcopy(topic_model)\n",
    "# topic_model_merged.set_topic_labels(topic_labels_dict)\n",
    "# topic_model_merged.merge_topics(docs, topics_to_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023_preprocessed.reset_index(drop=True, inplace=True)\n",
    "new_topics, new_probs = topic_model_merged.transform(df_2023_preprocessed['processed_data'])\n",
    "\n",
    "# Now, you can merge the new dataframe with the old one\n",
    "# df_labelled = pd.concat([df_labelled, df_2023_preprocessed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic and keywords for the new data\n",
    "new_topic_keywords = {}\n",
    "n = 10  # Number of keywords to include\n",
    "for topic in topic_model_merged.get_topics().keys():\n",
    "    topic_words = [word for word, _ in topic_model_merged.get_topic(topic)[:n]]\n",
    "    new_topic_keywords[topic] = \", \".join(topic_words)\n",
    "\n",
    "# Add the topics and keywords to the new dataframe\n",
    "df_2023_preprocessed['topic'] = new_topics\n",
    "df_2023_preprocessed['keywords'] = df_2023_preprocessed['topic'].map(new_topic_keywords)\n",
    "\n",
    "# Add the label column to the new dataframe\n",
    "mergedtopic_labels_dict = {i-1: item for i, item in enumerate(label_names_postmerge)}\n",
    "df_2023_preprocessed['label'] = df_2023_preprocessed['topic'].map(mergedtopic_labels_dict)\n",
    "\n",
    "# Get a list of the topics in the correct order, excluding topic -1\n",
    "topic_order = [topic for topic in topic_model_merged.get_topics().keys() if topic != -1]\n",
    "\n",
    "# Create a dictionary where the keys are the topic labels and the values are the probabilities\n",
    "new_proba_dict = []\n",
    "for proba in new_probs:\n",
    "    new_proba_dict.append(dict(sorted(zip(topic_order, proba), key=lambda item: item[1], reverse=True)))\n",
    "    \n",
    "# Add the proba_dict column to the new dataframe\n",
    "df_2023_preprocessed['proba_dict'] = new_proba_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can append the two dataframes, the 2023 data not null with the null rows from 2023_full\n",
    "df_final = df_2023_preprocessed.append(df_2023_preprocessed_full[df_2023_preprocessed_full['non_empty_rows'] == False], ignore_index=True)\n",
    "df_final['Response Date'] = pd.to_datetime(df_final['Response Date'])\n",
    "df_final['year_month'] = df_final['Response Date'].dt.to_period('M').dt.to_timestamp()\n",
    "df_final['Market Segment'] = df_final['Market Segment'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survey Type', 'Year'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_final.columns) - set(df_old.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "### Analyse de Sentiments simple : positif - négatif - neutre\n",
    "\n",
    "def load_model_huggingface(model_name, task, problem_type=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This function loads a model and tokenizer from a given model name, then creates a pipeline to perform a specified task.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "        task (str): The type of task to perform with the pipeline.\n",
    "        problem_type (str): The type of problem to solve (\"multi_label_classification\" for multi-label tasks).\n",
    "        **kwargs: Additional arguments to pass to the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        pipeline: A pipeline configured to perform the specified task with the loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=problem_type)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    classifier = pipeline(task, model=model, tokenizer=tokenizer, **kwargs)\n",
    "    return classifier\n",
    "\n",
    "def add_single_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function merges the DataFrame of single-label predictions with the original DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a dictionary containing a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Convert the predictions to a DataFrame\n",
    "    prediction_results = pd.DataFrame(predictions)\n",
    "    prediction_results.rename(columns={'label': predicted_column_name}, inplace=True)\n",
    "    # # Reset the indices of the DataFrames (if necessary)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # prediction_results.reset_index(drop=True, inplace=True)\n",
    "    # Merge the original DataFrame with the prediction results\n",
    "    df_predicted = pd.concat([predicted_df, prediction_results], axis=1)\n",
    "    return df_predicted\n",
    "\n",
    "def add_multi_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function adds a new column with multi-label predictions to the DataFrame and also adds two more columns for \n",
    "    the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a list of dictionaries, where each dictionary \n",
    "                            contains a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores, as well as \n",
    "                      columns for the best label and its score.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Keep the original predictions as they are (a list of dictionaries) and add them to the DataFrame as a new column\n",
    "    predicted_df[predicted_column_name] = predictions\n",
    "    # Add columns for the best label and its score\n",
    "    predicted_df[f'best_{predicted_column_name}'] = predicted_df[predicted_column_name].apply(lambda x: max(x.keys(), key=lambda k: x[k]) if x else None)\n",
    "    predicted_df[f'best_{predicted_column_name}_score'] = predicted_df[predicted_column_name].apply(lambda x: x[max(x.keys(), key=lambda k: x[k])] if x else None)\n",
    "    return predicted_df\n",
    "\n",
    "def make_predictions_df(classifier, df, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function makes predictions on a DataFrame of documents using a given classifier. It adds the predictions to \n",
    "    the DataFrame as new columns. If the classifier is for single-label classification, it adds one column for the \n",
    "    predicted label and one for the score. If the classifier is for multi-label classification, it adds one column \n",
    "    with a dictionary of label-score pairs for each document, and two additional columns for the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        classifier (pipeline): The Hugging Face pipeline object for making predictions.\n",
    "        df (pd.DataFrame): The DataFrame containing the documents to make predictions on. It must have a 'processed_data' \n",
    "                           column with the preprocessed text of each document.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame for the predictions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predictions.\n",
    "    \"\"\"\n",
    "    # Get the list of documents from the DataFrame\n",
    "    docs = df[\"processed_data\"].tolist()\n",
    "    # Get predictions\n",
    "    predictions = classifier(docs)\n",
    "    \n",
    "    # Check if predictions is a list of dictionaries (single-label case)\n",
    "    if isinstance(predictions, list) and isinstance(predictions[0], dict):\n",
    "        df_predicted = add_single_label_predictions(df, predictions, predicted_column_name)\n",
    "    \n",
    "    # Multi-label case\n",
    "    elif isinstance(predictions, list) and isinstance(predictions[0], list):\n",
    "        df_predicted = add_multi_label_predictions(df, predictions, predicted_column_name)\n",
    "\n",
    "    return df_predicted\n",
    "\n",
    "classifier = load_model_huggingface(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text-classification\", max_length=512, truncation=True)\n",
    "predicted_df = make_predictions_df(classifier, df_final, 'sentiment_label')\n",
    "\n",
    "predictions = []\n",
    "for review in [[x] for x in df_final.processed_data.tolist()]:\n",
    "      # Effectuer une prédiction pour le document en utilisant le classificateur\n",
    "    prediction = classifier(review)\n",
    "      # Ajouter la prédiction à la liste des prédictions\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Création d'un DataFrame à partir des résultats de prédiction\n",
    "res = pd.DataFrame([item for sublist in predictions for item in sublist])\n",
    "df_pred = pd.concat([df_final, res], axis=1)\n",
    "\n",
    "# The model created a column 'label' to store the prediction, BUT the df already had one column 'label', then :\n",
    "# Rename the second 'label' column to 'sentiment_label'\n",
    "cols = df_pred.columns.tolist()\n",
    "cols[len(cols) - 1 - cols[::-1].index('label')] = 'sentiment_label'\n",
    "df_pred.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ast\n",
    "\n",
    "### Analyse de Sentiments approfondie : par 28 émotions\n",
    "\n",
    "# en multilabel, avec GPU et par batch pour accélerer le traitement\n",
    "\n",
    "# Check if a CUDA-enabled GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=\"multi_label_classification\", max_length=512)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create a list of label names\n",
    "label_emotions = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise','neutral']\n",
    "\n",
    "# Initialize lists to store the predicted labels and scores\n",
    "predicted_labels = []\n",
    "predicted_scores = []\n",
    "\n",
    "df_emotion = df_pred\n",
    "# Iterate over the rows of the DataFrame in batches\n",
    "for i in range(0, len(df_emotion), batch_size):\n",
    "    batch = df_emotion[i:i+batch_size]\n",
    "    texts = batch['processed_data'].tolist()\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Move the inputs to the GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits.sigmoid().detach().cpu().numpy()\n",
    "    \n",
    "    # Apply a threshold to the probabilities to get the predicted labels\n",
    "    threshold = 0.5\n",
    "    labels = [[label_emotions[i] for i, prob in enumerate(prob_row) if prob > threshold] for prob_row in probs]\n",
    "    \n",
    "    # Store the predicted labels and scores\n",
    "    predicted_labels.extend(labels)\n",
    "    scores = [{label_emotions[i]: prob for i, prob in enumerate(prob_row)} for prob_row in probs]\n",
    "    # predicted_scores.extend(probs.tolist())\n",
    "    predicted_scores.extend(scores)\n",
    "\n",
    "# Add the predicted labels and scores as new columns in the DataFrame\n",
    "df_emotion['predicted_labels'] = predicted_labels\n",
    "df_emotion['predicted_scores'] = predicted_scores\n",
    "\n",
    "# # TO DO when reading the df, since the list columns will be read as full string and not list...\n",
    "# # Convert the string values in the 'predicted_labels' column into lists\n",
    "# df_emotion['predicted_labels'] = df_emotion['predicted_labels'].apply(lambda x: ast.literal_eval(x))\n",
    "# # Convert the string values in the 'predicted_scores' column into dictionaries\n",
    "# df_emotion['predicted_scores'] = df_emotion['predicted_scores'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict(input_dict, conversion_type='keys_to_values'):\n",
    "    \"\"\"\n",
    "    This function converts between a key-to-value dictionary and a value-to-keys dictionary.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): The input dictionary. If conversion_type is 'keys_to_values', this should be a dictionary where \n",
    "                           the keys are the original keys and the values are the corresponding values. If conversion_type is \n",
    "                           'values_to_keys', this should be a dictionary where the keys are the original values and the values \n",
    "                           are lists of keys for each value.\n",
    "        conversion_type (str): The type of conversion to perform. Can be either 'keys_to_values' or 'values_to_keys'.\n",
    "\n",
    "    Returns:\n",
    "        dict: The converted dictionary. If conversion_type is 'keys_to_values', this will be a dictionary where the keys \n",
    "              are the original values and the values are lists of keys for each value. If conversion_type is 'values_to_keys', \n",
    "              this will be a dictionary where the keys are the original keys and the values are the corresponding values.\n",
    "    \"\"\"\n",
    "    if conversion_type == 'keys_to_values':\n",
    "        output_dict = {}\n",
    "        for key, value in input_dict.items():\n",
    "            if value not in output_dict:\n",
    "                output_dict[value] = []\n",
    "            output_dict[value].append(key)\n",
    "    elif conversion_type == 'values_to_keys':\n",
    "        output_dict = {key: value for value, keys in input_dict.items() for key in keys}\n",
    "    else:\n",
    "        raise ValueError(\"Invalid conversion_type. Must be either 'keys_to_values' or 'values_to_keys'.\")\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def add_grouping_column(df, key_column, group_dict, group_column_name):\n",
    "    \"\"\"\n",
    "    This function adds a new column to a DataFrame with the group name of each key.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        key_column (str): The name of the column in df that contains the keys.\n",
    "        group_dict (dict): A dictionary where the keys are the original keys and the values are the corresponding group names.\n",
    "        group_column_name (str): The name of the new column to be added to df for the groups.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an added column for the groups.\n",
    "    \"\"\"\n",
    "    # df[group_column_name] = df[key_column].map(group_dict)\n",
    "    df[group_column_name] = df[key_column].apply(lambda labels: [group_dict[label] for label in labels if label in group_dict])\n",
    "    return df\n",
    "\n",
    "# transform the multilabel results from the emotions analysis in single label\n",
    "# Define the groups\n",
    "positive_emotions = ['admiration','approval','gratitude','caring','realization','joy','optimism','love','excitement','amusement','relief']\n",
    "negative_emotions = ['disappointment','disapproval','annoyance','confusion','nervousness','fear','sadness','remorse','disgust','embarrassment','anger']\n",
    "neutral_emotions = ['neutral','desire','surprise','curiosity']\n",
    "# Create a dictionary where the keys are the group names and the values are the lists of labels\n",
    "groups = {\"positive\": positive_emotions, \"negative\": negative_emotions, \"neutral\": neutral_emotions}\n",
    "# Convert the groups to a group_dict\n",
    "group_dict = convert_dict(groups, conversion_type='values_to_keys')\n",
    "\n",
    "# Now you can use group_dict with add_grouping_column\n",
    "# df_emotion = add_grouping_column(df_emotion, \"predicted_labels\", group_dict, \"sentiment_from_emotion_label\")\n",
    "\n",
    "# Create a new column 'single_emotion_label' that contains the label with the highest score\n",
    "df_emotion['single_emotion_label'] = df_emotion['predicted_scores'].apply(lambda x: max(x, key=x.get))\n",
    "# Create a new column 'sentiment' that contains the sentiment of the emotion in the 'single_emotion_label' column\n",
    "df_emotion['single_sentiment_from_emotion'] = df_emotion['single_emotion_label'].apply(lambda x: 'positive' if x in positive_emotions else ('negative' if x in negative_emotions else ('neutral' if x in neutral_emotions else 'unknown')))\n",
    "\n",
    "# df3.to_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_labels</th>\n",
       "      <th>predicted_scores</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>topic</th>\n",
       "      <th>label</th>\n",
       "      <th>keywords</th>\n",
       "      <th>single_emotion_label</th>\n",
       "      <th>single_sentiment_from_emotion</th>\n",
       "      <th>sentiment_from_emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>{'admiration': 0.001663754, 'amusement': 0.000...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Outliers</td>\n",
       "      <td>customer service, delivery time, sales, techni...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>{'admiration': 0.104325175, 'amusement': 0.000...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Outliers</td>\n",
       "      <td>customer service, delivery time, sales, techni...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>{'admiration': 0.0010179136, 'amusement': 0.00...</td>\n",
       "      <td>negative</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Delivery Deadlines</td>\n",
       "      <td>delivery time, delays, delivery date, deliveri...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[admiration]</td>\n",
       "      <td>{'admiration': 0.84221095, 'amusement': 0.0011...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Technical Support</td>\n",
       "      <td>technical support, response time, customer ser...</td>\n",
       "      <td>admiration</td>\n",
       "      <td>positive</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>{'admiration': 0.0072880113, 'amusement': 0.00...</td>\n",
       "      <td>positive</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Pricing</td>\n",
       "      <td>pricing, prices, discount, quotation, orders, ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[neutral]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predicted_labels                                   predicted_scores  \\\n",
       "0        [neutral]  {'admiration': 0.001663754, 'amusement': 0.000...   \n",
       "1               []  {'admiration': 0.104325175, 'amusement': 0.000...   \n",
       "2        [neutral]  {'admiration': 0.0010179136, 'amusement': 0.00...   \n",
       "3     [admiration]  {'admiration': 0.84221095, 'amusement': 0.0011...   \n",
       "4        [neutral]  {'admiration': 0.0072880113, 'amusement': 0.00...   \n",
       "\n",
       "  sentiment_label  topic               label  \\\n",
       "0         neutral   -1.0            Outliers   \n",
       "1         neutral   -1.0            Outliers   \n",
       "2        negative    2.0  Delivery Deadlines   \n",
       "3        positive    1.0   Technical Support   \n",
       "4        positive    7.0             Pricing   \n",
       "\n",
       "                                            keywords single_emotion_label  \\\n",
       "0  customer service, delivery time, sales, techni...              neutral   \n",
       "1  customer service, delivery time, sales, techni...              neutral   \n",
       "2  delivery time, delays, delivery date, deliveri...              neutral   \n",
       "3  technical support, response time, customer ser...           admiration   \n",
       "4  pricing, prices, discount, quotation, orders, ...              neutral   \n",
       "\n",
       "  single_sentiment_from_emotion sentiment_from_emotion_label  \n",
       "0                       neutral                    [neutral]  \n",
       "1                       neutral                           []  \n",
       "2                       neutral                    [neutral]  \n",
       "3                      positive                   [positive]  \n",
       "4                       neutral                    [neutral]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df_emotion[[\"predicted_labels\", 'predicted_scores', 'sentiment_label', 'topic','label','keywords','single_emotion_label','single_sentiment_from_emotion','sentiment_from_emotion_label']].head()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62843"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotion[\"allComment\"] = df_emotion.processed_data\n",
    "df_2023_full =df_emotion.append(df_old, ignore_index=True)\n",
    "len(df_2023_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion =  df_emotion[df_emotion.columns.difference(['Survey Type', 'Year'])]\n",
    "df_schneider_full = df_old.append(df_emotion, ignore_index=True)\n",
    "df_schneider_full.to_csv(\"../data/csv_files/schneider_all_processed_labelled_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20649"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_schneider_full['Likelihood to Recommend (SE)'].notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Cote D'Ivoire\", 'Iceland'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d(df_schneider_full['Zone'].dropna().unique(), df_old['Zone'].dropna().unique()) #['China', 'East Asia', 'Other Europe Operations']\n",
    "np.setdiff1d(df_schneider_full['Clusters'].dropna().unique(), df_old['Clusters'].dropna().unique()) #['Cluster KSAPYB', 'North Andean', 'Other Europe Operations','Other UK and Ireland', 'South Andean', 'Turkey Central Asia']\n",
    "np.setdiff1d(df_schneider_full['Account Country'].dropna().unique(), df_old['Account Country'].dropna().unique()) #[\"Cote D'Ivoire\", 'Iceland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.setdiff1d(df_old['Account Country'].dropna().unique(), df_schneider_full['Account Country'].dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5979/1866067157.py:16: DtypeWarning:\n",
      "\n",
      "Columns (5,6,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from itertools import cycle\n",
    "import plotly.subplots as sp\n",
    "import colorsys\n",
    "import streamlit as st \n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The loaded data.\n",
    "    \"\"\"\n",
    "    # data = pd.read_csv('../data/csv_files/schneider_processed_labelled_full.csv')\n",
    "    # data = pd.read_csv('../data/csv_files/schneider_all_processed_labelled_full.csv')\n",
    "    data = pd.read_csv('../data/csv_files/schneider_all_processed_labelled_full_newBertopic.csv')\n",
    "    return data\n",
    "\n",
    "data = load_data()\n",
    "data_comments_only = data[data['non_empty_rows']]\n",
    "groupby_options = ['year', 'Zone', 'Clusters','Account Country', 'Market Segment']\n",
    "# Store the parameters lists in a dictionary\n",
    "my_data = {option: (np.insert(data[option].unique().astype('object'), 0, \"all_time\") if option == 'year' else data[option].unique()) for option in groupby_options}\n",
    "# Add the merged_topics and emotions lists to the my_data dictionary\n",
    "my_data['merged_topics'] = data['label'].dropna().unique()\n",
    "my_data['emotions'] = data['single_emotion_label'].dropna().unique()\n",
    "\n",
    "levels = ['Zone','Clusters','Account Country'] # it has to be from the highest level to the lowest\n",
    "color_sequence = ['#636EFA','#EF553B','#00CC96','#AB63FA','#FFA15A','#19D3F3','#FF6692','#B6E880','#FF97FF','#FECB52','#E763FA','#BA68C8','#FFA000','#F06292','#7986CB','#4DB6AC','#FF8A65','#A1887F','#90A4AE','#E53935','#8E24AA']\n",
    "\n",
    "# sunburst chart functions \n",
    "def filter_docs(df, filter_column, filter_value):\n",
    "    filter_mask = df[filter_column] == filter_value\n",
    "    return df[filter_mask], filter_mask\n",
    "\n",
    "def compute_labels_and_parents(data_df, levels):\n",
    "    grouped = data_df.groupby(levels).size()\n",
    "\n",
    "    # Initialize lists\n",
    "    labels = []\n",
    "    parents = []\n",
    "\n",
    "    # Compute labels and parents lists\n",
    "    for i in range(len(levels)):\n",
    "        for label, value in grouped.items(): # loop over the rows\n",
    "            if label[i] not in labels:\n",
    "                labels.append(label[i]) \n",
    "                parents.append(\"\") if label[i] in data_df[levels[0]].values else parents.append(label[i-1])\n",
    "    \n",
    "    return labels, parents\n",
    "\n",
    "def prepare_dataframe(data_df, levels):\n",
    "    df_tmp=data_df.copy()\n",
    "    frequency_col_name = \"Count\"\n",
    "    df_tmp[frequency_col_name] = df_tmp.groupby(levels[-1])[levels[-1]].transform('count')\n",
    "    sunburst_df = df_tmp[levels + [frequency_col_name]].drop_duplicates()\n",
    "    sunburst_df[frequency_col_name] = sunburst_df[frequency_col_name].fillna(0)\n",
    "    \n",
    "    return sunburst_df\n",
    "\n",
    "def compute_values(sunburst_df, labels, levels):\n",
    "    values = []\n",
    "    processed_labels = set()  # This set will store the labels that have been processed\n",
    "    for label in labels:\n",
    "        if label not in processed_labels:  # Only process the label if it has not been processed before\n",
    "            for level in levels:\n",
    "                if label in sunburst_df[level].values:\n",
    "                    value = sunburst_df.loc[sunburst_df[level] == label, 'Count'].sum()\n",
    "                    values.append(value)\n",
    "                    processed_labels.add(label)  # Add the label to the set of processed labels\n",
    "                    break  # No need to check other levels for this label\n",
    "                    \n",
    "    return values\n",
    "\n",
    "def get_level(label, data_df, levels, return_highest_level=True): \n",
    "    # Find the levels that the label appears in\n",
    "    found_levels = [level for level in levels if label in data_df[level].values]\n",
    "    if not found_levels:\n",
    "        # raise ValueError(f\"Label {label} not found\")\n",
    "        return \"Unknown\"\n",
    "    # If return_highest is True, return the first level found (highest), else return the last level found (lowest)\n",
    "    if return_highest_level:\n",
    "        return found_levels[0]\n",
    "    else:\n",
    "        return found_levels[-1]\n",
    "\n",
    "def compute_percentages(sunburst_df, data_df, labels, parents, values, levels):\n",
    "    percentages = []\n",
    "    percentages_class = []\n",
    "    processed_labels = set()  # This set will store the labels that have been processed\n",
    "    \n",
    "    for level in levels:\n",
    "        # Compute total count for each label in current level\n",
    "        level_counts = sunburst_df.groupby(level)['Count'].sum()\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in processed_labels and label in sunburst_df[level].values:\n",
    "                label_count = sunburst_df.loc[sunburst_df[level] == label, 'Count'].sum() \n",
    "\n",
    "                # Compute total count for the parent of current label with a condition if the label has a parent or not. \n",
    "                # Without parent, the total count is the count for all labels at this highest level\n",
    "                total_count = values[labels.index(parents[i])] if parents[i]!='' else level_counts.sum()\n",
    "                # Compute total count from original dataframe \n",
    "                total_count_class = data_df[get_level(label, data_df, levels)].value_counts().get(label)                \n",
    "\n",
    "                # Compute percentage for current label\n",
    "                percentage = (label_count / total_count) * 100\n",
    "                percentages.append(percentage)\n",
    "                percentage_class = (label_count / total_count_class) * 100\n",
    "                percentages_class.append(percentage_class)\n",
    "                processed_labels.add(label)  # Add the label to the set of processed labels\n",
    "\n",
    "    return percentages, percentages_class\n",
    "\n",
    "def transform_scale_list(lst,exponent=2, desire_sum=100):\n",
    "    # Apply power transformation\n",
    "    transformed_lst = [i**exponent for i in lst]\n",
    "\n",
    "    # Scale the transformed data so that the total sum remains 100\n",
    "    total = sum(transformed_lst)\n",
    "    scaled_lst = [(i/total)*desire_sum for i in transformed_lst]\n",
    "    return scaled_lst\n",
    "\n",
    "def normalize_percentage_class(data_df, levels, labels, parents, percentages_class, scale_exponent=2):\n",
    "    # Init\n",
    "    normalized_percentages_class = [0]*len(percentages_class)\n",
    "    normalized_percentages_class_transform = [0]*len(percentages_class)\n",
    "\n",
    "    ##### Highest level\n",
    "\n",
    "    # Compute normalized_percentage_class for labels at the highest level (without parent)\n",
    "    highest_level_labels = [label for label in labels if get_level(label, data_df, levels) == levels[0]]\n",
    "    highest_level_indices = [labels.index(label) for label in highest_level_labels]\n",
    "    total_highest_level_percentage = sum(percentages_class[i] for i in highest_level_indices)\n",
    "    for i in highest_level_indices:\n",
    "        normalized_percentages_class[i] = percentages_class[i] / total_highest_level_percentage * 100\n",
    "\n",
    "    # scale the values for the highest level\n",
    "    normalized_percentages_class_transform = transform_scale_list(normalized_percentages_class,scale_exponent)\n",
    "\n",
    "    ##### Other levels\n",
    "\n",
    "    # Normalize percentages_class based on its parent\n",
    "    for level in levels[1:]: #loop on the levels except the highest one\n",
    "        level_labels = [label for label in labels if get_level(label, data_df, levels) == level]\n",
    "        level_indices = [labels.index(label) for label in level_labels]\n",
    "\n",
    "        # Loop to create the normalized_percentages_class list by level\n",
    "        for indice in level_indices:\n",
    "            sibling_indices = [j for j, parent in enumerate(parents) if parent == parents[indice]]\n",
    "            total_sibling_percentage = sum(percentages_class[j] for j in sibling_indices)\n",
    "            normalized_percentage_class = percentages_class[indice] / total_sibling_percentage * normalized_percentages_class[labels.index(parents[indice])]\n",
    "            normalized_percentages_class[indice] = normalized_percentage_class\n",
    "\n",
    "        # Loop to transform the normalized_percentages_class list by level : list normalized_percentages_class_transform\n",
    "        indices_updated = [] # list to avoid several treatments on each label\n",
    "        for indice in level_indices:\n",
    "            if indice not in indices_updated:\n",
    "                sibling_indices = [j for j, parent in enumerate(parents) if parent == parents[indice]]\n",
    "                # Get & Transform norm_pct_class by family (brothers with the same parent)\n",
    "                norm_pct_class_sibling = [normalized_percentages_class[i] for i in sibling_indices]\n",
    "                norm_pct_class_sibling_transform = transform_scale_list(norm_pct_class_sibling, scale_exponent, desire_sum=normalized_percentages_class_transform[labels.index(parents[indice])])\n",
    "                # Store the transform norm_pct_class in the normalized_percentages_class_transform list\n",
    "                for i, sibling_ind in enumerate(sibling_indices):\n",
    "                    normalized_percentages_class_transform[sibling_ind] = norm_pct_class_sibling_transform[i]\n",
    "                    indices_updated.append(sibling_ind)\n",
    "\n",
    "    return normalized_percentages_class_transform\n",
    "    \n",
    "def lighten_color(color, factor):\n",
    "    # Check if the input is RGB or hexadecimal\n",
    "    if isinstance(color, tuple):\n",
    "        r, g, b = color\n",
    "    else:\n",
    "        # Remove the '#' from the start of the color code if it exists\n",
    "        if color.startswith('#'):\n",
    "            color = color[1:]\n",
    "        r, g, b = tuple(int(color[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "    # Convert RGB color to HLS\n",
    "    h, l, s = colorsys.rgb_to_hls(r/255.0, g/255.0, b/255.0)\n",
    "    # Increase the lightness\n",
    "    l = max(min(l + factor, 1.0), 0.0)\n",
    "    # Convert back to RGB\n",
    "    r, g, b = colorsys.hls_to_rgb(h, l, s)\n",
    "    \n",
    "    # Convert back to the original format\n",
    "    if isinstance(color, tuple):\n",
    "        return int(r*255), int(g*255), int(b*255)\n",
    "    else:\n",
    "        return '#%02x%02x%02x' % (int(r*255), int(g*255), int(b*255))\n",
    "    \n",
    "def compute_color_list(data_df, labels, parents, levels, color_sequence):\n",
    "    # Create a dictionary that maps each label to a color\n",
    "    color_dict = {}\n",
    "\n",
    "    # Sort your labels based on their level\n",
    "    labels_sorted = sorted(labels, key=lambda label: get_level(label, data_df, levels), reverse=True)\n",
    "    color_seq = color_sequence.copy()\n",
    "    # Iterate over your reversed sorted labels list\n",
    "    for i, label in enumerate(labels_sorted):\n",
    "        parent = parents[labels.index(label)]  # Get the parent of the current label\n",
    "        level = get_level(label, data_df, levels)  # Use your get_level function here\n",
    "\n",
    "        if level == levels[0]:\n",
    "            # Assign a unique color to each continent\n",
    "            if label not in color_dict:\n",
    "                # color_dict[label] = color_seq.pop(0)\n",
    "                color_dict[label] = color_sequence[i % len(color_sequence)]\n",
    "        else:\n",
    "            # Assign a lighter shade of the parent's color to each country or city\n",
    "            parent_color = color_dict[parent]\n",
    "            color_dict[label] = lighten_color(parent_color, 0.1)\n",
    "    return color_dict \n",
    "\n",
    "def add_unique_parent(labels, parents, values, unique_parent_name):\n",
    "    unique_parent_value = sum(values[i] for i in range(len(parents)) if parents[i] == '')\n",
    "    for i in range(len(labels)):\n",
    "        if parents[i] == '':\n",
    "            parents[i] = unique_parent_name\n",
    "    labels.append(unique_parent_name)\n",
    "    parents.append('')\n",
    "    values.append(unique_parent_value)\n",
    "\n",
    "    return labels, parents, values\n",
    "\n",
    "def compute_lists(data_df, levels, color_sequence, class_column=None, class_value=None, unique_parent=False, unique_parent_name=\"Geo Levels\"):\n",
    "    if class_column and class_value==None:\n",
    "        raise ValueError(\"When class_column is defined, class_value must be defined too and must be a value from the class_column column\")\n",
    "    \n",
    "    work_df = data_df.copy()\n",
    "    if class_column:\n",
    "        work_df = filter_docs(data_df, class_column, class_value)[0]\n",
    "    \n",
    "    labels, parents = compute_labels_and_parents(work_df, levels)\n",
    "    sunburst_df = prepare_dataframe(work_df, levels)\n",
    "    values = compute_values(sunburst_df, labels, levels)\n",
    "    # percentages, percentages_class, normalized_percentages_class = compute_percentages(sunburst_df, data_df, labels, parents, values, levels)\n",
    "    percentages, percentages_class = compute_percentages(sunburst_df, data_df, labels, parents, values, levels)\n",
    "    normalized_percentages_class_transform = normalize_percentage_class(data_df, levels, labels, parents, percentages_class, scale_exponent=1)\n",
    "    color_dict = compute_color_list(data_df, labels, parents, levels, color_sequence)\n",
    "    if unique_parent:\n",
    "        labels, parents, values =  add_unique_parent(labels, parents, values, unique_parent_name=unique_parent_name)\n",
    "        # Compute and add percentages for the unique_parent label\n",
    "        percentages.append(100)\n",
    "        percentages_class.append(len(filter_docs(data_df, class_column, class_value)[0])/len(data_df)*100) if class_column and class_value else percentages_class.append(\"None\")\n",
    "        # normalized_percentages_class.append(100) if class_column and class_value else normalized_percentages_class.append(\"None\")\n",
    "        normalized_percentages_class_transform.append(100) if class_column and class_value else normalized_percentages_class_transform.append(\"None\")\n",
    "        color_dict[unique_parent_name]='#ffffff'\n",
    "    return labels, parents, values, percentages, percentages_class, normalized_percentages_class_transform, color_dict\n",
    "\n",
    "def create_sunburst_fig(labels, parents, values, percentages, percentages_class, normalized_percentages_class_transform, color_dict, class_column=None, class_value=None, **sunburst_kwargs):\n",
    "    # Create a list of hover texts that includes the percentage for each label\n",
    "    hover_text = [f'{label}<br>Number of verbatims: {value}<br>Percentage: {percentage:.2f}%{f\"<br>Percentage of {class_value}: {percentage_class:.2f}%\" if class_column else \"\"}' for label, value, percentage, percentage_class in zip(labels, values, percentages, percentages_class)]\n",
    "\n",
    "    fig = go.Figure(data=go.Sunburst(\n",
    "        labels=labels,\n",
    "        parents=parents,\n",
    "        values=normalized_percentages_class_transform if class_column else values,\n",
    "        branchvalues='total',\n",
    "        hovertext=hover_text,\n",
    "        hovertemplate='%{hovertext}<extra></extra>',  # Only use custom hover text and remove extra info            \n",
    "        marker=dict(colors=[color_dict[label] for label in labels]),  # Set colors based on your mapping\n",
    "        # insidetextorientation='radial',  # Make labels orient radially\n",
    "        **sunburst_kwargs\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title={'text': f\"Geographical Distribution\",\n",
    "               'y':0.95,'x':0.5,\n",
    "               'xanchor': 'center','yanchor': 'top'},\n",
    "        title_font=dict(size=20,\n",
    "                        color='rgb(107, 107, 107)'),\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def sunburst(data_df, levels, color_sequence, unique_parent=True, class_column=None, class_value=None, **sunburst_kwargs):\n",
    "    labels, parents, values, percentages, percentages_class, normalized_percentages_class_transform, color_dict = compute_lists(data_df, levels, color_sequence, unique_parent=unique_parent, class_column=class_column, class_value=class_value)\n",
    "    fig = create_sunburst_fig(labels, parents, values, percentages, percentages_class, normalized_percentages_class_transform, color_dict, class_column=class_column, class_value=class_value, **sunburst_kwargs)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "branchvalues": "total",
         "hovertemplate": "%{hovertext}<extra></extra>",
         "hovertext": [
          "BeNe<br>Number of verbatims: 1731<br>Percentage: 4.40%",
          "CEEI<br>Number of verbatims: 2181<br>Percentage: 5.55%",
          "Canada<br>Number of verbatims: 1021<br>Percentage: 2.60%",
          "China & HK<br>Number of verbatims: 1998<br>Percentage: 5.08%",
          "DACH<br>Number of verbatims: 4791<br>Percentage: 12.18%",
          "East Asia Japan<br>Number of verbatims: 5431<br>Percentage: 13.81%",
          "France<br>Number of verbatims: 2133<br>Percentage: 5.42%",
          "Greater India<br>Number of verbatims: 1483<br>Percentage: 3.77%",
          "Iberia<br>Number of verbatims: 2444<br>Percentage: 6.22%",
          "Italy<br>Number of verbatims: 4638<br>Percentage: 11.80%",
          "Mexico & Central America<br>Number of verbatims: 31<br>Percentage: 0.08%",
          "Middle East and Africa<br>Number of verbatims: 1587<br>Percentage: 4.04%",
          "Nordic & Baltics<br>Number of verbatims: 2430<br>Percentage: 6.18%",
          "Pacific<br>Number of verbatims: 1355<br>Percentage: 3.45%",
          "South America<br>Number of verbatims: 2432<br>Percentage: 6.19%",
          "UK and Ireland<br>Number of verbatims: 984<br>Percentage: 2.50%",
          "US<br>Number of verbatims: 2650<br>Percentage: 6.74%",
          "Belgium<br>Number of verbatims: 779<br>Percentage: 45.00%",
          "Netherlands<br>Number of verbatims: 952<br>Percentage: 55.00%",
          "Israel<br>Number of verbatims: 200<br>Percentage: 9.17%",
          "Middle Eastern Europe<br>Number of verbatims: 1728<br>Percentage: 79.23%",
          "Southeast Europe<br>Number of verbatims: 253<br>Percentage: 11.60%",
          "China<br>Number of verbatims: 1991<br>Percentage: 99.65%",
          "Hong Kong & Macao<br>Number of verbatims: 7<br>Percentage: 0.35%",
          "Austria<br>Number of verbatims: 529<br>Percentage: 11.04%",
          "Germany<br>Number of verbatims: 2933<br>Percentage: 61.22%",
          "Liechtenstein<br>Number of verbatims: 1<br>Percentage: 0.02%",
          "Switzerland<br>Number of verbatims: 1328<br>Percentage: 27.72%",
          "North East Asia<br>Number of verbatims: 4290<br>Percentage: 78.99%",
          "South East Asia<br>Number of verbatims: 1141<br>Percentage: 21.01%",
          "India<br>Number of verbatims: 1483<br>Percentage: 100.00%",
          "Portugal<br>Number of verbatims: 451<br>Percentage: 18.45%",
          "Spain<br>Number of verbatims: 1993<br>Percentage: 81.55%",
          "Mexico<br>Number of verbatims: 31<br>Percentage: 100.00%",
          "Anglophone Africa<br>Number of verbatims: 254<br>Percentage: 16.01%",
          "Francophone Africa<br>Number of verbatims: 5<br>Percentage: 0.32%",
          "Gulf<br>Number of verbatims: 21<br>Percentage: 1.32%",
          "North East Africa and Levant<br>Number of verbatims: 58<br>Percentage: 3.65%",
          "Saudi Arabia & Yemen<br>Number of verbatims: 122<br>Percentage: 7.69%",
          "Turkey Central Asia and Pakistan<br>Number of verbatims: 1127<br>Percentage: 71.01%",
          "Denmark<br>Number of verbatims: 642<br>Percentage: 26.42%",
          "Finland & Baltics<br>Number of verbatims: 696<br>Percentage: 28.64%",
          "Norway<br>Number of verbatims: 447<br>Percentage: 18.40%",
          "Sweden<br>Number of verbatims: 645<br>Percentage: 26.54%",
          "Australia<br>Number of verbatims: 992<br>Percentage: 73.21%",
          "New Zealand<br>Number of verbatims: 363<br>Percentage: 26.79%",
          "Andean Cluster<br>Number of verbatims: 141<br>Percentage: 5.80%",
          "Argentina, Uruguay and Paraguay<br>Number of verbatims: 436<br>Percentage: 17.93%",
          "Brazil<br>Number of verbatims: 1855<br>Percentage: 76.27%",
          "Ireland<br>Number of verbatims: 18<br>Percentage: 1.83%",
          "United Kingdom<br>Number of verbatims: 966<br>Percentage: 98.17%",
          "USA<br>Number of verbatims: 2650<br>Percentage: 100.00%",
          "Czech Republic<br>Number of verbatims: 687<br>Percentage: 39.76%",
          "Poland<br>Number of verbatims: 773<br>Percentage: 44.73%",
          "Slovakia<br>Number of verbatims: 268<br>Percentage: 15.51%",
          "Bulgaria<br>Number of verbatims: 21<br>Percentage: 8.30%",
          "Croatia<br>Number of verbatims: 20<br>Percentage: 7.91%",
          "Greece<br>Number of verbatims: 14<br>Percentage: 5.53%",
          "Hungary<br>Number of verbatims: 48<br>Percentage: 18.97%",
          "Romania<br>Number of verbatims: 140<br>Percentage: 55.34%",
          "Serbia<br>Number of verbatims: 7<br>Percentage: 2.77%",
          "Slovenia<br>Number of verbatims: 3<br>Percentage: 1.19%",
          "Hong Kong<br>Number of verbatims: 7<br>Percentage: 100.00%",
          "Japan<br>Number of verbatims: 2999<br>Percentage: 69.91%",
          "Korea, Republic of<br>Number of verbatims: 853<br>Percentage: 19.88%",
          "Taiwan<br>Number of verbatims: 438<br>Percentage: 10.21%",
          "Indonesia<br>Number of verbatims: 312<br>Percentage: 27.34%",
          "Malaysia<br>Number of verbatims: 44<br>Percentage: 3.86%",
          "Singapore<br>Number of verbatims: 294<br>Percentage: 25.77%",
          "Thailand<br>Number of verbatims: 372<br>Percentage: 32.60%",
          "Vietnam<br>Number of verbatims: 119<br>Percentage: 10.43%",
          "San Marino<br>Number of verbatims: 2<br>Percentage: 0.04%",
          "South Africa<br>Number of verbatims: 254<br>Percentage: 100.00%",
          "Dominican Republic<br>Number of verbatims: 1<br>Percentage: 20.00%",
          "Morocco<br>Number of verbatims: 4<br>Percentage: 80.00%",
          "Kuwait<br>Number of verbatims: 3<br>Percentage: 14.29%",
          "Oman<br>Number of verbatims: 1<br>Percentage: 4.76%",
          "Qatar<br>Number of verbatims: 2<br>Percentage: 9.52%",
          "United Arab Emirates<br>Number of verbatims: 15<br>Percentage: 71.43%",
          "Egypt<br>Number of verbatims: 55<br>Percentage: 94.83%",
          "Jordan<br>Number of verbatims: 1<br>Percentage: 1.72%",
          "Lebanon<br>Number of verbatims: 2<br>Percentage: 3.45%",
          "Saudi Arabia<br>Number of verbatims: 122<br>Percentage: 100.00%",
          "Kazakhstan<br>Number of verbatims: 76<br>Percentage: 6.74%",
          "Pakistan<br>Number of verbatims: 10<br>Percentage: 0.89%",
          "Turkey<br>Number of verbatims: 1041<br>Percentage: 92.37%",
          "Estonia<br>Number of verbatims: 12<br>Percentage: 1.72%",
          "Finland<br>Number of verbatims: 643<br>Percentage: 92.39%",
          "Latvia<br>Number of verbatims: 19<br>Percentage: 2.73%",
          "Lithuania<br>Number of verbatims: 22<br>Percentage: 3.16%",
          "Chile<br>Number of verbatims: 86<br>Percentage: 60.99%",
          "Colombia<br>Number of verbatims: 44<br>Percentage: 31.21%",
          "Peru<br>Number of verbatims: 11<br>Percentage: 7.80%",
          "Argentina<br>Number of verbatims: 434<br>Percentage: 99.54%",
          "Paraguay<br>Number of verbatims: 2<br>Percentage: 0.46%",
          "Geo Levels<br>Number of verbatims: 39320<br>Percentage: 100.00%"
         ],
         "labels": [
          "BeNe",
          "CEEI",
          "Canada",
          "China & HK",
          "DACH",
          "East Asia Japan",
          "France",
          "Greater India",
          "Iberia",
          "Italy",
          "Mexico & Central America",
          "Middle East and Africa",
          "Nordic & Baltics",
          "Pacific",
          "South America",
          "UK and Ireland",
          "US",
          "Belgium",
          "Netherlands",
          "Israel",
          "Middle Eastern Europe",
          "Southeast Europe",
          "China",
          "Hong Kong & Macao",
          "Austria",
          "Germany",
          "Liechtenstein",
          "Switzerland",
          "North East Asia",
          "South East Asia",
          "India",
          "Portugal",
          "Spain",
          "Mexico",
          "Anglophone Africa",
          "Francophone Africa",
          "Gulf",
          "North East Africa and Levant",
          "Saudi Arabia & Yemen",
          "Turkey Central Asia and Pakistan",
          "Denmark",
          "Finland & Baltics",
          "Norway",
          "Sweden",
          "Australia",
          "New Zealand",
          "Andean Cluster",
          "Argentina, Uruguay and Paraguay",
          "Brazil",
          "Ireland",
          "United Kingdom",
          "USA",
          "Czech Republic",
          "Poland",
          "Slovakia",
          "Bulgaria",
          "Croatia",
          "Greece",
          "Hungary",
          "Romania",
          "Serbia",
          "Slovenia",
          "Hong Kong",
          "Japan",
          "Korea, Republic of",
          "Taiwan",
          "Indonesia",
          "Malaysia",
          "Singapore",
          "Thailand",
          "Vietnam",
          "San Marino",
          "South Africa",
          "Dominican Republic",
          "Morocco",
          "Kuwait",
          "Oman",
          "Qatar",
          "United Arab Emirates",
          "Egypt",
          "Jordan",
          "Lebanon",
          "Saudi Arabia",
          "Kazakhstan",
          "Pakistan",
          "Turkey",
          "Estonia",
          "Finland",
          "Latvia",
          "Lithuania",
          "Chile",
          "Colombia",
          "Peru",
          "Argentina",
          "Paraguay",
          "Geo Levels"
         ],
         "marker": {
          "colors": [
           "#636EFA",
           "#EF553B",
           "#00CC96",
           "#AB63FA",
           "#FFA15A",
           "#19D3F3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52",
           "#E763FA",
           "#BA68C8",
           "#FFA000",
           "#F06292",
           "#7986CB",
           "#4DB6AC",
           "#FF8A65",
           "#949bfb",
           "#949bfb",
           "#f27d6a",
           "#f27d6a",
           "#f27d6a",
           "#c594fb",
           "#c594fb",
           "#febe8d",
           "#febe8d",
           "#febe8d",
           "#febe8d",
           "#49dcf5",
           "#49dcf5",
           "#ceefab",
           "#fecafe",
           "#fecafe",
           "#ee94fb",
           "#cb8dd5",
           "#cb8dd5",
           "#cb8dd5",
           "#cb8dd5",
           "#cb8dd5",
           "#cb8dd5",
           "#ffb332",
           "#ffb332",
           "#ffb332",
           "#ffb332",
           "#f490b2",
           "#f490b2",
           "#9da7d9",
           "#9da7d9",
           "#9da7d9",
           "#71c4bc",
           "#71c4bc",
           "#feb098",
           "#f6a598",
           "#f6a598",
           "#f6a598",
           "#f6a598",
           "#f6a598",
           "#f6a598",
           "#f6a598",
           "#f6a598",
           "#f6a598",
           "#f6a598",
           "#dfc5fc",
           "#79e5f7",
           "#79e5f7",
           "#79e5f7",
           "#79e5f7",
           "#79e5f7",
           "#79e5f7",
           "#79e5f7",
           "#79e5f7",
           "#feda84",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#dbb2e2",
           "#ffc564",
           "#ffc564",
           "#ffc564",
           "#ffc564",
           "#c1c8e7",
           "#c1c8e7",
           "#c1c8e7",
           "#c1c8e7",
           "#c1c8e7",
           "#ffffff"
          ]
         },
         "parents": [
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "Geo Levels",
          "BeNe",
          "BeNe",
          "CEEI",
          "CEEI",
          "CEEI",
          "China & HK",
          "China & HK",
          "DACH",
          "DACH",
          "DACH",
          "DACH",
          "East Asia Japan",
          "East Asia Japan",
          "Greater India",
          "Iberia",
          "Iberia",
          "Mexico & Central America",
          "Middle East and Africa",
          "Middle East and Africa",
          "Middle East and Africa",
          "Middle East and Africa",
          "Middle East and Africa",
          "Middle East and Africa",
          "Nordic & Baltics",
          "Nordic & Baltics",
          "Nordic & Baltics",
          "Nordic & Baltics",
          "Pacific",
          "Pacific",
          "South America",
          "South America",
          "South America",
          "UK and Ireland",
          "UK and Ireland",
          "US",
          "Middle Eastern Europe",
          "Middle Eastern Europe",
          "Middle Eastern Europe",
          "Southeast Europe",
          "Southeast Europe",
          "Southeast Europe",
          "Southeast Europe",
          "Southeast Europe",
          "Southeast Europe",
          "Southeast Europe",
          "Hong Kong & Macao",
          "North East Asia",
          "North East Asia",
          "North East Asia",
          "South East Asia",
          "South East Asia",
          "South East Asia",
          "South East Asia",
          "South East Asia",
          "Italy",
          "Anglophone Africa",
          "Francophone Africa",
          "Francophone Africa",
          "Gulf",
          "Gulf",
          "Gulf",
          "Gulf",
          "North East Africa and Levant",
          "North East Africa and Levant",
          "North East Africa and Levant",
          "Saudi Arabia & Yemen",
          "Turkey Central Asia and Pakistan",
          "Turkey Central Asia and Pakistan",
          "Turkey Central Asia and Pakistan",
          "Finland & Baltics",
          "Finland & Baltics",
          "Finland & Baltics",
          "Finland & Baltics",
          "Andean Cluster",
          "Andean Cluster",
          "Andean Cluster",
          "Argentina, Uruguay and Paraguay",
          "Argentina, Uruguay and Paraguay",
          ""
         ],
         "type": "sunburst",
         "values": [
          1731,
          2181,
          1021,
          1998,
          4791,
          5431,
          2133,
          1483,
          2444,
          4638,
          31,
          1587,
          2430,
          1355,
          2432,
          984,
          2650,
          779,
          952,
          200,
          1728,
          253,
          1991,
          7,
          529,
          2933,
          1,
          1328,
          4290,
          1141,
          1483,
          451,
          1993,
          31,
          254,
          5,
          21,
          58,
          122,
          1127,
          642,
          696,
          447,
          645,
          992,
          363,
          141,
          436,
          1855,
          18,
          966,
          2650,
          687,
          773,
          268,
          21,
          20,
          14,
          48,
          140,
          7,
          3,
          7,
          2999,
          853,
          438,
          312,
          44,
          294,
          372,
          119,
          2,
          254,
          1,
          4,
          3,
          1,
          2,
          15,
          55,
          1,
          2,
          122,
          76,
          10,
          1041,
          12,
          643,
          19,
          22,
          86,
          44,
          11,
          434,
          2,
          39320
         ]
        }
       ],
       "layout": {
        "height": 1000,
        "template": {
         "data": {
          "candlestick": [
           {
            "decreasing": {
             "line": {
              "color": "#000033"
             }
            },
            "increasing": {
             "line": {
              "color": "#000032"
             }
            },
            "type": "candlestick"
           }
          ],
          "contour": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram2d": [
           {
            "colorscale": [
             [
              0,
              "#000011"
             ],
             [
              0.1111111111111111,
              "#000012"
             ],
             [
              0.2222222222222222,
              "#000013"
             ],
             [
              0.3333333333333333,
              "#000014"
             ],
             [
              0.4444444444444444,
              "#000015"
             ],
             [
              0.5555555555555556,
              "#000016"
             ],
             [
              0.6666666666666666,
              "#000017"
             ],
             [
              0.7777777777777778,
              "#000018"
             ],
             [
              0.8888888888888888,
              "#000019"
             ],
             [
              1,
              "#000020"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "icicle": [
           {
            "textfont": {
             "color": "white"
            },
            "type": "icicle"
           }
          ],
          "sankey": [
           {
            "textfont": {
             "color": "#000036"
            },
            "type": "sankey"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "width": 0
             }
            },
            "type": "scatter"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#000038"
             },
             "font": {
              "color": "#000037"
             },
             "line": {
              "color": "#000039"
             }
            },
            "header": {
             "fill": {
              "color": "#000040"
             },
             "font": {
              "color": "#000036"
             },
             "line": {
              "color": "#000039"
             }
            },
            "type": "table"
           }
          ],
          "waterfall": [
           {
            "connector": {
             "line": {
              "color": "#000036",
              "width": 2
             }
            },
            "decreasing": {
             "marker": {
              "color": "#000033"
             }
            },
            "increasing": {
             "marker": {
              "color": "#000032"
             }
            },
            "totals": {
             "marker": {
              "color": "#000034"
             }
            },
            "type": "waterfall"
           }
          ]
         },
         "layout": {
          "coloraxis": {
           "colorscale": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ]
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#000021"
            ],
            [
             0.1,
             "#000022"
            ],
            [
             0.2,
             "#000023"
            ],
            [
             0.3,
             "#000024"
            ],
            [
             0.4,
             "#000025"
            ],
            [
             0.5,
             "#000026"
            ],
            [
             0.6,
             "#000027"
            ],
            [
             0.7,
             "#000028"
            ],
            [
             0.8,
             "#000029"
            ],
            [
             0.9,
             "#000030"
            ],
            [
             1,
             "#000031"
            ]
           ],
           "sequential": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#000011"
            ],
            [
             0.1111111111111111,
             "#000012"
            ],
            [
             0.2222222222222222,
             "#000013"
            ],
            [
             0.3333333333333333,
             "#000014"
            ],
            [
             0.4444444444444444,
             "#000015"
            ],
            [
             0.5555555555555556,
             "#000016"
            ],
            [
             0.6666666666666666,
             "#000017"
            ],
            [
             0.7777777777777778,
             "#000018"
            ],
            [
             0.8888888888888888,
             "#000019"
            ],
            [
             1,
             "#000020"
            ]
           ]
          },
          "colorway": [
           "#000001",
           "#000002",
           "#000003",
           "#000004",
           "#000005",
           "#000006",
           "#000007",
           "#000008",
           "#000009",
           "#000010"
          ]
         }
        },
        "title": {
         "font": {
          "color": "rgb(107, 107, 107)",
          "size": 20
         },
         "text": "Geographical Distribution",
         "x": 0.5,
         "xanchor": "center",
         "y": 0.95,
         "yanchor": "top"
        },
        "width": 1000
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sunburst(data_comments_only, levels, color_sequence, unique_parent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wassati",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
