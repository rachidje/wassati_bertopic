{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6977/2036508806.py:5: DtypeWarning: Columns (10,11,15,19,20,21,23,34,35,42,43,44,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\", dtype={'year': str})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_all_labelled.csv\", dtype={'year': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_huggingface(model_name, task, problem_type=None, **kwargs):\n",
    "    \"\"\"\n",
    "    This function loads a model and tokenizer from a given model name, then creates a pipeline to perform a specified task.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "        task (str): The type of task to perform with the pipeline.\n",
    "        problem_type (str): The type of problem to solve (\"multi_label_classification\" for multi-label tasks).\n",
    "        **kwargs: Additional arguments to pass to the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        pipeline: A pipeline configured to perform the specified task with the loaded model and tokenizer.\n",
    "    \"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=problem_type)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    classifier = pipeline(task, model=model, tokenizer=tokenizer, **kwargs)\n",
    "    return classifier\n",
    "\n",
    "def add_single_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function merges the DataFrame of single-label predictions with the original DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a dictionary containing a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Convert the predictions to a DataFrame\n",
    "    prediction_results = pd.DataFrame(predictions)\n",
    "    prediction_results.rename(columns={'label': predicted_column_name}, inplace=True)\n",
    "    # # Reset the indices of the DataFrames (if necessary)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    # prediction_results.reset_index(drop=True, inplace=True)\n",
    "    # Merge the original DataFrame with the prediction results\n",
    "    df_predicted = pd.concat([predicted_df, prediction_results], axis=1)\n",
    "    return df_predicted\n",
    "\n",
    "def add_multi_label_predictions(df, predictions, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function adds a new column with multi-label predictions to the DataFrame and also adds two more columns for \n",
    "    the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        predictions (list): The list of predictions. Each prediction is a list of dictionaries, where each dictionary \n",
    "                            contains a 'label' and a 'score'.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predicted labels and their scores, as well as \n",
    "                      columns for the best label and its score.\n",
    "    \"\"\"\n",
    "    predicted_df = df\n",
    "    # Keep the original predictions as they are (a list of dictionaries) and add them to the DataFrame as a new column\n",
    "    predicted_df[predicted_column_name] = predictions\n",
    "    # Add columns for the best label and its score\n",
    "    predicted_df[f'best_{predicted_column_name}'] = predicted_df[predicted_column_name].apply(lambda x: max(x.keys(), key=lambda k: x[k]) if x else None)\n",
    "    predicted_df[f'best_{predicted_column_name}_score'] = predicted_df[predicted_column_name].apply(lambda x: x[max(x.keys(), key=lambda k: x[k])] if x else None)\n",
    "    return predicted_df\n",
    "\n",
    "def make_predictions_df(classifier, df, predicted_column_name):\n",
    "    \"\"\"\n",
    "    This function makes predictions on a DataFrame of documents using a given classifier. It adds the predictions to \n",
    "    the DataFrame as new columns. If the classifier is for single-label classification, it adds one column for the \n",
    "    predicted label and one for the score. If the classifier is for multi-label classification, it adds one column \n",
    "    with a dictionary of label-score pairs for each document, and two additional columns for the best label and its score.\n",
    "\n",
    "    Args:\n",
    "        classifier (pipeline): The Hugging Face pipeline object for making predictions.\n",
    "        df (pd.DataFrame): The DataFrame containing the documents to make predictions on. It must have a 'processed_data' \n",
    "                           column with the preprocessed text of each document.\n",
    "        predicted_column_name (str): The name of the column to be added to the DataFrame for the predictions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with added columns for the predictions.\n",
    "    \"\"\"\n",
    "    # Get the list of documents from the DataFrame\n",
    "    docs = df[\"processed_data\"].tolist()\n",
    "    # Get predictions\n",
    "    predictions = classifier(docs)\n",
    "    \n",
    "    # Check if predictions is a list of dictionaries (single-label case)\n",
    "    if isinstance(predictions, list) and isinstance(predictions[0], dict):\n",
    "        df_predicted = add_single_label_predictions(df, predictions, predicted_column_name)\n",
    "    \n",
    "    # Multi-label case\n",
    "    elif isinstance(predictions, list) and isinstance(predictions[0], list):\n",
    "        df_predicted = add_multi_label_predictions(df, predictions, predicted_column_name)\n",
    "\n",
    "    return df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = load_model_huggingface(\"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"text-classification\", max_length=512, truncation=True)\n",
    "# predicted_df = make_predictions_df(classifier, df, 'sentiment_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = load_model_huggingface(\"tum-nlp/Deberta_Human_Value_Detector\", \"text-classification\", max_length=512, truncation=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer =  AutoTokenizer.from_pretrained(\"tum-nlp/Deberta_Human_Value_Detector\")\n",
    "# trained_model = AutoModelForSequenceClassification.from_pretrained(\"tum-nlp/Deberta_Human_Value_Detector\", trust_remote_code=True)\n",
    "\n",
    "# example_text ='We should ban whaling because whales are a species at the risk of distinction'\n",
    "\n",
    "# encoding = tokenizer.encode_plus(\n",
    "#         example_text,\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=512,\n",
    "#         return_token_type_ids=False,\n",
    "#         padding=\"max_length\",\n",
    "#         return_attention_mask=True,\n",
    "#         return_tensors='pt',\n",
    "#     )\n",
    "\n",
    "# with torch.no_grad():\n",
    "#         test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "#         test_prediction = test_prediction[\"output\"].flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "Universalism: nature: 0.9919975399971008\n"
     ]
    }
   ],
   "source": [
    "# THRESHOLD = 0.25\n",
    "# LABEL_COLUMNS = ['Self-direction: thought','Self-direction: action','Stimulation','Hedonism','Achievement','Power: dominance','Power: resources','Face','Security: personal','Security: societal','Tradition','Conformity: rules','Conformity: interpersonal','Humility','Benevolence: caring','Benevolence: dependability','Universalism: concern','Universalism: nature','Universalism: tolerance','Universalism: objectivity']\n",
    "# print(f\"Predictions:\")\n",
    "# for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "#     if prediction < THRESHOLD:\n",
    "#         continue\n",
    "#     print(f\"{label}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tum-nlp/Deberta_Human_Value_Detector\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"tum-nlp/Deberta_Human_Value_Detector\", trust_remote_code=True)\n",
    "\n",
    "# Define the threshold and label columns\n",
    "THRESHOLD = 0.25\n",
    "LABEL_COLUMNS = ['Self-direction: thought','Self-direction: action','Stimulation','Hedonism','Achievement','Power: dominance','Power: resources','Face','Security: personal',\n",
    "                 'Security: societal','Tradition','Conformity: rules','Conformity: interpersonal','Humility','Benevolence: caring','Benevolence: dependability','Universalism: concern','Universalism: nature','Universalism: tolerance','Universalism: objectivity']\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Move the model to the GPU if available\n",
    "model.to(device)\n",
    "\n",
    "def predict_values(row):\n",
    "    text = row['processed_data']    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        return_token_type_ids=False,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    # Move the tensors to the GPU if available\n",
    "    encoding = {key: tensor.to(device) for key, tensor in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "        predictions = predictions[\"output\"].flatten()\n",
    "\n",
    "    # Move the predictions back to CPU and convert to numpy\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    labels_scores = [(label, prediction) for label, prediction in zip(LABEL_COLUMNS, predictions) if prediction >= THRESHOLD]\n",
    "    \n",
    "    return labels_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df2['predictions'] = df2.apply(predict_values, axis=1)\n",
    "\n",
    "# Explode the predictions into separate rows\n",
    "df2 = df2.explode('predictions')\n",
    "\n",
    "# Split the predictions into two separate columns\n",
    "df2[['schwartz_label', 'schwartz_score']] = pd.DataFrame(df2['predictions'].tolist(), index=df2.index)\n",
    "\n",
    "# Drop the predictions column\n",
    "df2 = df2.drop(columns='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Achievement                   33824\n",
       "Universalism: objectivity     26631\n",
       "Benevolence: caring           25461\n",
       "Self-direction: action        14192\n",
       "Security: personal            11726\n",
       "Benevolence: dependability     9745\n",
       "Power: resources               2448\n",
       "Conformity: rules              2339\n",
       "Face                           2280\n",
       "Self-direction: thought        2104\n",
       "Conformity: interpersonal      1929\n",
       "Power: dominance               1524\n",
       "Security: societal              609\n",
       "Universalism: concern           473\n",
       "Hedonism                        247\n",
       "Universalism: tolerance         172\n",
       "Universalism: nature            157\n",
       "Tradition                       111\n",
       "Humility                        110\n",
       "Stimulation                     109\n",
       "Name: schwartz_label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['schwartz_label'].value_counts()\n",
    "# retrieve the texts with the highest scores for each label\n",
    "# df.loc[df.groupby('schwartz_label')['schwartz_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"/media/cattiaux/DATA/Wassati/team_data/schneider/df_schwartz_labelled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136274"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136274"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account Country</th>\n",
       "      <th>Clusters</th>\n",
       "      <th>Zone</th>\n",
       "      <th>Market Segment</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>Creation Date</th>\n",
       "      <th>year</th>\n",
       "      <th>year_month</th>\n",
       "      <th>Likelihood to Recommend (SE)</th>\n",
       "      <th>Overall Satisfaction</th>\n",
       "      <th>allComment</th>\n",
       "      <th>processed_data</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>single_emotion_label</th>\n",
       "      <th>single_sentiment_from_emotion</th>\n",
       "      <th>score</th>\n",
       "      <th>predicted_labels</th>\n",
       "      <th>predicted_scores</th>\n",
       "      <th>Found Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Israel</td>\n",
       "      <td>Israel</td>\n",
       "      <td>CEEI</td>\n",
       "      <td>Semiconductor</td>\n",
       "      <td>zoop_823001254</td>\n",
       "      <td>2023-05-11 09:10:06</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>excellent equipment, high responsiveness and s...</td>\n",
       "      <td>excellent equipment, high responsiveness and s...</td>\n",
       "      <td>Outlier</td>\n",
       "      <td>negative</td>\n",
       "      <td>disappointment</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.898911</td>\n",
       "      <td>['disappointment']</td>\n",
       "      <td>{'admiration': 0.026408667, 'amusement': 0.001...</td>\n",
       "      <td>[energy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>DACH</td>\n",
       "      <td>Automotive &amp; E-Mobility</td>\n",
       "      <td>zoop_823001147</td>\n",
       "      <td>2023-05-09 17:15:05</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>schneider electric offers good advice to solve...</td>\n",
       "      <td>schneider electric offers good advice to solve...</td>\n",
       "      <td>Outlier</td>\n",
       "      <td>positive</td>\n",
       "      <td>approval</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.798206</td>\n",
       "      <td>['approval']</td>\n",
       "      <td>{'admiration': 0.08126737, 'amusement': 0.0004...</td>\n",
       "      <td>[energy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Pacific</td>\n",
       "      <td>Power &amp; Grid</td>\n",
       "      <td>OP-230207-12877737_Q</td>\n",
       "      <td>2023-02-16 23:32:30</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>product was required urgently, sales/support t...</td>\n",
       "      <td>product was required urgently, sales/support t...</td>\n",
       "      <td>Outlier</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.522348</td>\n",
       "      <td>['neutral']</td>\n",
       "      <td>{'admiration': 0.0011441872, 'amusement': 0.00...</td>\n",
       "      <td>[energy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>Finland</td>\n",
       "      <td>Finland &amp; Baltics</td>\n",
       "      <td>Nordic &amp; Baltics</td>\n",
       "      <td>MMM</td>\n",
       "      <td>OP-221020-12570971_Q</td>\n",
       "      <td>2022-12-07 23:33:01</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>energy efficiency and energy saving have a hig...</td>\n",
       "      <td>energy efficiency and energy saving have a hig...</td>\n",
       "      <td>Outlier</td>\n",
       "      <td>positive</td>\n",
       "      <td>approval</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.587052</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'admiration': 0.08122513, 'amusement': 0.0005...</td>\n",
       "      <td>[energy, energy saving, energy efficiency]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>Turkey Central Asia and Pakistan</td>\n",
       "      <td>Middle East and Africa</td>\n",
       "      <td>Machinery</td>\n",
       "      <td>5008V00001Tj3vSQAR</td>\n",
       "      <td>2022-11-17 23:31:20</td>\n",
       "      <td>2022</td>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>thanks to mehmet ali̇ for their support, and t...</td>\n",
       "      <td>thanks to mehmet ali̇ for their support, and t...</td>\n",
       "      <td>Automation Components</td>\n",
       "      <td>positive</td>\n",
       "      <td>gratitude</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.954546</td>\n",
       "      <td>['gratitude']</td>\n",
       "      <td>{'admiration': 0.01158437, 'amusement': 0.0023...</td>\n",
       "      <td>[energy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31326</th>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>zoop_422200637</td>\n",
       "      <td>2018-06-30 00:19:37</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>the expertise provided to me is good for my fi...</td>\n",
       "      <td>the expertise provided to me is good for my fi...</td>\n",
       "      <td>Automation Components</td>\n",
       "      <td>positive</td>\n",
       "      <td>approval</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.891485</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'admiration': 0.31609055, 'amusement': 0.0005...</td>\n",
       "      <td>[energy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31484</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>zoop_421969920</td>\n",
       "      <td>2018-06-30 00:19:37</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-05-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>the service is very, very slow and the product...</td>\n",
       "      <td>the service is very, very slow and the product...</td>\n",
       "      <td>Automation Components</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.936458</td>\n",
       "      <td>['neutral']</td>\n",
       "      <td>{'admiration': 0.0014525086, 'amusement': 0.00...</td>\n",
       "      <td>[energy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31754</th>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>China &amp; HK</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>zoop_420078798</td>\n",
       "      <td>2018-06-30 00:19:37</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-04-01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>we have purchased more than 10 to 20 million's...</td>\n",
       "      <td>we have purchased more than 10 to 20 million's...</td>\n",
       "      <td>Outlier</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.704671</td>\n",
       "      <td>['neutral']</td>\n",
       "      <td>{'admiration': 0.008560948, 'amusement': 0.000...</td>\n",
       "      <td>[energy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33070</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>North East Africa and Levant</td>\n",
       "      <td>Middle East and Africa</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>zoop_355802772</td>\n",
       "      <td>2018-06-30 00:39:48</td>\n",
       "      <td>2018</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>احنا المفروض ان احنا مصنعين لمنتجات شنايدر لوح...</td>\n",
       "      <td>احنا المفروض ان احنا مصنعين لمنتجات شنايدر لوح...</td>\n",
       "      <td>Power Supply Issues</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.595841</td>\n",
       "      <td>['neutral']</td>\n",
       "      <td>{'admiration': 0.0013201903, 'amusement': 0.00...</td>\n",
       "      <td>[solar power]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33466</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>Nordic &amp; Baltics</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>zoop_355816986</td>\n",
       "      <td>2018-06-30 00:39:48</td>\n",
       "      <td>2018</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>i think it runs very well. the products are re...</td>\n",
       "      <td>i think it runs very well. the products are re...</td>\n",
       "      <td>Product Evaluation</td>\n",
       "      <td>positive</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.655506</td>\n",
       "      <td>['annoyance']</td>\n",
       "      <td>{'admiration': 0.027035365, 'amusement': 0.002...</td>\n",
       "      <td>[energy]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Account Country                          Clusters  \\\n",
       "499            Israel                            Israel   \n",
       "501       Switzerland                       Switzerland   \n",
       "1191        Australia                         Australia   \n",
       "1864          Finland                 Finland & Baltics   \n",
       "2042           Turkey  Turkey Central Asia and Pakistan   \n",
       "...               ...                               ...   \n",
       "31326          France                            France   \n",
       "31484          Canada                            Canada   \n",
       "31754           China                             China   \n",
       "33070           Egypt      North East Africa and Levant   \n",
       "33466         Denmark                           Denmark   \n",
       "\n",
       "                         Zone           Market Segment             Unique ID  \\\n",
       "499                      CEEI            Semiconductor        zoop_823001254   \n",
       "501                      DACH  Automotive & E-Mobility        zoop_823001147   \n",
       "1191                  Pacific             Power & Grid  OP-230207-12877737_Q   \n",
       "1864         Nordic & Baltics                      MMM  OP-221020-12570971_Q   \n",
       "2042   Middle East and Africa                Machinery    5008V00001Tj3vSQAR   \n",
       "...                       ...                      ...                   ...   \n",
       "31326                  France                  Unknown        zoop_422200637   \n",
       "31484                  Canada                  Unknown        zoop_421969920   \n",
       "31754              China & HK                  Unknown        zoop_420078798   \n",
       "33070  Middle East and Africa                  Unknown        zoop_355802772   \n",
       "33466        Nordic & Baltics                  Unknown        zoop_355816986   \n",
       "\n",
       "             Creation Date  year  year_month  Likelihood to Recommend (SE)  \\\n",
       "499    2023-05-11 09:10:06  2023  2023-05-01                           8.0   \n",
       "501    2023-05-09 17:15:05  2023  2023-05-01                           8.0   \n",
       "1191   2023-02-16 23:32:30  2023  2023-02-01                           NaN   \n",
       "1864   2022-12-07 23:33:01  2022  2022-12-01                           NaN   \n",
       "2042   2022-11-17 23:31:20  2022  2022-11-01                           NaN   \n",
       "...                    ...   ...         ...                           ...   \n",
       "31326  2018-06-30 00:19:37  2018  2018-05-01                           9.0   \n",
       "31484  2018-06-30 00:19:37  2018  2018-05-01                           4.0   \n",
       "31754  2018-06-30 00:19:37  2018  2018-04-01                          10.0   \n",
       "33070  2018-06-30 00:39:48  2018  2017-12-01                          10.0   \n",
       "33466  2018-06-30 00:39:48  2018  2017-12-01                           8.0   \n",
       "\n",
       "       Overall Satisfaction  \\\n",
       "499                     8.0   \n",
       "501                     8.0   \n",
       "1191                    1.0   \n",
       "1864                    8.0   \n",
       "2042                   10.0   \n",
       "...                     ...   \n",
       "31326                   9.0   \n",
       "31484                   4.0   \n",
       "31754                  10.0   \n",
       "33070                  10.0   \n",
       "33466                   8.0   \n",
       "\n",
       "                                              allComment  \\\n",
       "499    excellent equipment, high responsiveness and s...   \n",
       "501    schneider electric offers good advice to solve...   \n",
       "1191   product was required urgently, sales/support t...   \n",
       "1864   energy efficiency and energy saving have a hig...   \n",
       "2042   thanks to mehmet ali̇ for their support, and t...   \n",
       "...                                                  ...   \n",
       "31326  the expertise provided to me is good for my fi...   \n",
       "31484  the service is very, very slow and the product...   \n",
       "31754  we have purchased more than 10 to 20 million's...   \n",
       "33070  احنا المفروض ان احنا مصنعين لمنتجات شنايدر لوح...   \n",
       "33466  i think it runs very well. the products are re...   \n",
       "\n",
       "                                          processed_data  \\\n",
       "499    excellent equipment, high responsiveness and s...   \n",
       "501    schneider electric offers good advice to solve...   \n",
       "1191   product was required urgently, sales/support t...   \n",
       "1864   energy efficiency and energy saving have a hig...   \n",
       "2042   thanks to mehmet ali̇ for their support, and t...   \n",
       "...                                                  ...   \n",
       "31326  the expertise provided to me is good for my fi...   \n",
       "31484  the service is very, very slow and the product...   \n",
       "31754  we have purchased more than 10 to 20 million's...   \n",
       "33070  احنا المفروض ان احنا مصنعين لمنتجات شنايدر لوح...   \n",
       "33466  i think it runs very well. the products are re...   \n",
       "\n",
       "                       label sentiment_label single_emotion_label  \\\n",
       "499                  Outlier        negative       disappointment   \n",
       "501                  Outlier        positive             approval   \n",
       "1191                 Outlier        negative              neutral   \n",
       "1864                 Outlier        positive             approval   \n",
       "2042   Automation Components        positive            gratitude   \n",
       "...                      ...             ...                  ...   \n",
       "31326  Automation Components        positive             approval   \n",
       "31484  Automation Components        negative              neutral   \n",
       "31754                Outlier        positive              neutral   \n",
       "33070    Power Supply Issues         neutral              neutral   \n",
       "33466     Product Evaluation        positive            annoyance   \n",
       "\n",
       "      single_sentiment_from_emotion     score    predicted_labels  \\\n",
       "499                        negative  0.898911  ['disappointment']   \n",
       "501                        positive  0.798206        ['approval']   \n",
       "1191                        neutral  0.522348         ['neutral']   \n",
       "1864                       positive  0.587052                  []   \n",
       "2042                       positive  0.954546       ['gratitude']   \n",
       "...                             ...       ...                 ...   \n",
       "31326                      positive  0.891485                  []   \n",
       "31484                       neutral  0.936458         ['neutral']   \n",
       "31754                       neutral  0.704671         ['neutral']   \n",
       "33070                       neutral  0.595841         ['neutral']   \n",
       "33466                      negative  0.655506       ['annoyance']   \n",
       "\n",
       "                                        predicted_scores  \\\n",
       "499    {'admiration': 0.026408667, 'amusement': 0.001...   \n",
       "501    {'admiration': 0.08126737, 'amusement': 0.0004...   \n",
       "1191   {'admiration': 0.0011441872, 'amusement': 0.00...   \n",
       "1864   {'admiration': 0.08122513, 'amusement': 0.0005...   \n",
       "2042   {'admiration': 0.01158437, 'amusement': 0.0023...   \n",
       "...                                                  ...   \n",
       "31326  {'admiration': 0.31609055, 'amusement': 0.0005...   \n",
       "31484  {'admiration': 0.0014525086, 'amusement': 0.00...   \n",
       "31754  {'admiration': 0.008560948, 'amusement': 0.000...   \n",
       "33070  {'admiration': 0.0013201903, 'amusement': 0.00...   \n",
       "33466  {'admiration': 0.027035365, 'amusement': 0.002...   \n",
       "\n",
       "                                      Found Terms  \n",
       "499                                      [energy]  \n",
       "501                                      [energy]  \n",
       "1191                                     [energy]  \n",
       "1864   [energy, energy saving, energy efficiency]  \n",
       "2042                                     [energy]  \n",
       "...                                           ...  \n",
       "31326                                    [energy]  \n",
       "31484                                    [energy]  \n",
       "31754                                    [energy]  \n",
       "33070                               [solar power]  \n",
       "33466                                    [energy]  \n",
       "\n",
       "[62 rows x 20 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search_terms_in_df(df, column, terms):\n",
    "    \"\"\"\n",
    "    Search for multiple terms in a specific column in a DataFrame and add a new column with the found terms.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to search.\n",
    "    column (str): The column in which to search for the terms.\n",
    "    terms (list): The list of terms to search for.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing only the rows where any of the terms were found, with an additional column 'Found Terms'.\n",
    "    \"\"\"\n",
    "    df_res = df.copy()\n",
    "    mask = df[column].apply(lambda x: [term for term in terms if term.lower() in str(x).lower()])\n",
    "    df_res['Found Terms'] = mask\n",
    "    df_res = df_res[df_res['Found Terms'].apply(lambda x: len(x) > 0)]\n",
    "    return df_res\n",
    "\n",
    "terms = [\n",
    "    'carbon', \n",
    "    'energy', \n",
    "    'energy saving', \n",
    "    'renewable energy', \n",
    "    'solar power', \n",
    "    'wind energy', \n",
    "    'hydroelectric power', \n",
    "    'geothermal energy', \n",
    "    'bioenergy', \n",
    "    'energy efficiency', \n",
    "    'carbon footprint', \n",
    "    'greenhouse gas emissions', \n",
    "    'climate change',\n",
    "    'sustainability', \n",
    "    'conservation'\n",
    "]\n",
    "\n",
    "useful_col = [\"Account Country\", \"Clusters\", \"Zone\", \"Market Segment\", \"Unique ID\", \"Creation Date\", \"year\", \"year_month\", \"Likelihood to Recommend (SE)\", \"Overall Satisfaction\", \"allComment\", \"processed_data\", \"label\", \"sentiment_label\", \"single_emotion_label\", \"single_sentiment_from_emotion\", \"score\",\"predicted_labels\", \"predicted_scores\"]\n",
    "search_terms_in_df(df[useful_col], 'processed_data', terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wassati",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
