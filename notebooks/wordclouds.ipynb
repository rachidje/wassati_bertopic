{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import nltk\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.colors import ListedColormap\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from bertopic import BERTopic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19840/2145852517.py:1: DtypeWarning: Columns (5,6,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/csv_files/schneider_all_processed_labelled_full_newBertopic.csv\", dtype={'year': str})\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/csv_files/schneider_all_processed_labelled_full_newBertopic.csv\", dtype={'year': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cattiaux/anaconda3/envs/wassati/lib/python3.9/site-packages/bertopic/vectorizers/_ctfidf.py:82: RuntimeWarning: divide by zero encountered in divide\n",
      "  idf = np.log((avg_nr_samples / df)+1)\n"
     ]
    }
   ],
   "source": [
    "### chargement du model bertopic\n",
    "\n",
    "def load_bertopic_model(filename):\n",
    "    \"\"\"\n",
    "    Load a BERTopic model and associated data from a file.\n",
    "    \n",
    "    :param filename: The name of the file to load the data from.\n",
    "    :return: A tuple containing the loaded BERTopic model, topics, probs, and docs variables.\n",
    "    \"\"\"\n",
    "    # Load the BERTopic model\n",
    "    topic_model = BERTopic.load(filename)\n",
    "    \n",
    "    # Load the topics, probs, and docs variables\n",
    "    with open(filename + '_data.pkl', 'rb') as f:\n",
    "        topics, probs, embeddings, docs = pickle.load(f)\n",
    "    \n",
    "    return topic_model, topics, probs, embeddings, docs\n",
    "\n",
    "topic_model, topics, probs, embeddings, docs = load_bertopic_model('../models/schneider_bertopic_model_2023')\n",
    "\n",
    "def create_merged_model(docs, bertopic_model, topics_to_merge_dict, label_names_dict):\n",
    "    \"\"\"\n",
    "    Create a new BERTopic model by merging topics from an existing model.\n",
    "\n",
    "    This function takes as input a list of documents `docs`, an existing BERTopic model `bertopic_model`, a dictionary `topics_to_merge_dict` specifying which topics to merge, and a dictionary `label_names_dict` specifying the labels for the merged topics.\n",
    "\n",
    "    The function creates a deep copy of the input BERTopic model and merges the specified topics using the `merge_topics` method. Then, it sets the topic labels for the merged model using the `set_topic_labels` method and the provided `label_names_dict`.\n",
    "\n",
    "    The resulting merged BERTopic model is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        bertopic_model (BERTopic): The input BERTopic model to be merged.\n",
    "        topics_to_merge_dict (dict): A dictionary specifying which topics to merge. The keys are the topic numbers to be merged, and the values are the topic numbers into which they should be merged.\n",
    "        label_names_dict (dict): A dictionary specifying the labels for the merged topics. The keys are the topic numbers, and the values are the corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        BERTopic: The resulting merged BERTopic model.\n",
    "    \"\"\"\n",
    "    topic_model_merged = copy.deepcopy(bertopic_model)\n",
    "    topic_model_merged.merge_topics(docs, topics_to_merge_dict)\n",
    "\n",
    "    # Create a dictionary to match the aggregated name to their corresponding topic number\n",
    "    mergedtopic_labels_dict = {i-1: item for i, item in enumerate(label_names_dict)}\n",
    "    # Set topic labels for the aggregated model\n",
    "    topic_model_merged.set_topic_labels(mergedtopic_labels_dict)\n",
    "\n",
    "    return topic_model_merged### Création du modèle bertopic aggrégé pour topics finaux\n",
    "\n",
    "# List of topics numbers. Each value of this list is a list that contains the topic number of the topics to join together\n",
    "topics_to_merge = [                     \n",
    "                    [0,1,4,8,11,20], # Product Quality and Sales\n",
    "                    [9,14], # Inverters and Drives Support : about providing support and service for inverters and drives\n",
    "                    [23,5,12,6,7], # Technical Support and Maintenance for UPS Systems and Touch Panels\n",
    "                    [13,15], # Quotations and Offers Management\n",
    "                    [29,27,16,22], # Customer Service and Delivery Scheduling\n",
    "                    [2,3,10,17,25,21], # Technical Support and Problem Resolution\n",
    "                    [24,18,30,31,19,32,28,26] # Communication, Guidance and Feedback\n",
    "]\n",
    "\n",
    "# Set the topic names for the new aggregated topic\n",
    "label_names = [\n",
    "    \"Outliers\",\n",
    "    \"Product Quality & Sales\",\n",
    "    \"Tech Support & Problem Resolution\",\n",
    "    \"UPS & Touch Panels\",\n",
    "    \"Communication, Guidance & Feedback\",\n",
    "    \"Inverters & Drives Support\",\n",
    "    \"Quotations & Offers\",\n",
    "    \"Customer Service & Delivery\"\n",
    "    ]\n",
    "\n",
    "# Create a new merged bertopic model \n",
    "topic_model_merged = create_merged_model(docs, topic_model, topics_to_merge, label_names)\n",
    "# topic_model_merged.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/cattiaux/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/cattiaux/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/cattiaux/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### Wordcloud\n",
    "### Wordcloud \n",
    "# Lemmatize keywords and recompute their probabilities for the wordcloud image\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def lemmatize_words(topic_words):\n",
    "    \"\"\"\n",
    "    Lemmatize the words and combine their probabilities.\n",
    "\n",
    "    This function takes as input a list of tuples `topic_words`, where each tuple contains a word and its probability. The function lemmatizes each word using the WordNetLemmatizer from the NLTK library, and combines the probabilities of the lemmas and their inflected forms.\n",
    "\n",
    "    The resulting dictionary, where the keys are the lemmas and the values are their combined probabilities, is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        topic_words (list): A list of tuples, where each tuple contains a word (str) and its probability (float).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the lemmas (str) and the values are their combined probabilities (float).\n",
    "    \"\"\"\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        \"\"\"\n",
    "        Convert NLTK part of speech tags to WordNet tags.\n",
    "\n",
    "        This function takes as input a part of speech tag in the format used by the NLTK library and returns the corresponding WordNet tag. The mapping between NLTK and WordNet tags is as follows:\n",
    "        - 'J' (adjective) maps to `wordnet.ADJ`\n",
    "        - 'V' (verb) maps to `wordnet.VERB`\n",
    "        - 'N' (noun) maps to `wordnet.NOUN`\n",
    "        - 'R' (adverb) maps to `wordnet.ADV`\n",
    "        - All other tags map to `wordnet.NOUN`\n",
    "\n",
    "        Parameters:\n",
    "            treebank_tag (str): The NLTK part of speech tag to be converted.\n",
    "\n",
    "        Returns:\n",
    "            str: The corresponding WordNet part of speech tag.\n",
    "        \"\"\"\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    # Create a lemmatizer object\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Create a dictionary to store the lemmas and their probabilities\n",
    "    lemma_prob = {}\n",
    "\n",
    "    # Lemmatize each word and combine their probabilities\n",
    "    for word, prob in topic_words:\n",
    "        # Tokenize the word and get its part of speech\n",
    "        tokens = nltk.word_tokenize(word)\n",
    "        pos = nltk.pos_tag(tokens)[0][1]\n",
    "        # Get the WordNet part of speech tag\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        # Lemmatize the word\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        \n",
    "        # Combine the probabilities of the lemma and its inflected forms\n",
    "        if lemma in lemma_prob:\n",
    "            lemma_prob[lemma] += prob\n",
    "        else:\n",
    "            lemma_prob[lemma] = prob\n",
    "    \n",
    "    return lemma_prob\n",
    "\n",
    "def recalculate_probabilities(lemma_prob, docs, topic_model):\n",
    "    \"\"\"\n",
    "    Recalculate the c-TF-IDF scores for the lemmas.\n",
    "\n",
    "    This function takes as input a dictionary `lemma_prob` containing the lemmas and their probabilities, a list of documents `docs`, and a BERTopic model `topic_model`. The function recalculates the c-TF-IDF scores for the lemmas using the provided documents and BERTopic model.\n",
    "\n",
    "    The function first calculates the term frequencies for each lemma using the CountVectorizer from the BERTopic model. Then, it calculates the inverse document frequencies for each lemma and uses these values to compute the c-TF-IDF scores. The c-TF-IDF scores are then normalized and used to update the probabilities of the lemmas.\n",
    "\n",
    "    The resulting dictionary, where the keys are the lemmas and the values are their updated probabilities, is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        lemma_prob (dict): A dictionary where the keys are the lemmas (str) and the values are their probabilities (float).\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        topic_model (BERTopic): The BERTopic model used to calculate the c-TF-IDF scores.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the lemmas (str) and the values are their updated probabilities (float).\n",
    "    \"\"\"\n",
    "    # Calculate the term frequencies using the provided CountVectorizer\n",
    "    X = topic_model.vectorizer_model.transform(docs)\n",
    "    \n",
    "    # Calculate the term frequencies for each lemma\n",
    "    tf = {}\n",
    "    for lemma, prob in lemma_prob.items():\n",
    "        index = topic_model.vectorizer_model.vocabulary_.get(lemma)\n",
    "        if index is not None:\n",
    "            tf[lemma] = np.sum(X[:, index])\n",
    "\n",
    "    # Calculate the inverse document frequencies for each lemma\n",
    "    df = np.sum(X > 0, axis=0)\n",
    "    N = X.shape[0]\n",
    "    idf = np.log(N / (df + 1))\n",
    "\n",
    "    # Calculate the c-TF-IDF scores for each lemma and normalize them\n",
    "    c_tf_idf = {}\n",
    "    for lemma, prob in lemma_prob.items():\n",
    "        index = topic_model.vectorizer_model.vocabulary_.get(lemma)\n",
    "        if index is not None:\n",
    "            c_tf_idf[lemma] = tf[lemma] * idf[0, index]\n",
    "    c_tf_idf_sum = np.sum(list(c_tf_idf.values()))\n",
    "    for lemma, score in c_tf_idf.items():\n",
    "        c_tf_idf[lemma] /= c_tf_idf_sum\n",
    "\n",
    "    # Update the probabilities of the lemmas based on their c-TF-IDF scores\n",
    "    new_lemma_prob = {}\n",
    "    for lemma, prob in lemma_prob.items():\n",
    "        if lemma in c_tf_idf:\n",
    "            new_lemma_prob[lemma] = c_tf_idf[lemma]\n",
    "    \n",
    "    return new_lemma_prob\n",
    "\n",
    "def get_topic_words(bertopic_model, topic, top_n=10):\n",
    "    \"\"\"\n",
    "    Get the top n words for a given topic.\n",
    "\n",
    "    This function takes as input a BERTopic model `topic_model`, a topic number `topic`, and an optional integer parameter `top_n` specifying the number of words to return. The function returns the top n words for the given topic, along with their probabilities, as a list of tuples.\n",
    "\n",
    "    The function first retrieves the c-TF-IDF matrix and feature names from the BERTopic model. Then, it gets the row of the c-TF-IDF matrix corresponding to the given topic and uses it to find the indices of the top n words. Finally, it retrieves the words and their probabilities and returns them as a list of tuples.\n",
    "\n",
    "    Parameters:\n",
    "        topic_model (BERTopic): The BERTopic model used to calculate the topic words.\n",
    "        topic (int): The topic number for which to retrieve the top n words.\n",
    "        top_n (int): An optional integer parameter specifying the number of words to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains a word (str) and its probability (float).\n",
    "    \"\"\"\n",
    "    # Get the c-TF-IDF matrix and feature names\n",
    "    c_tf_idf = bertopic_model.c_tf_idf_.toarray()\n",
    "    feature_names = bertopic_model.vectorizer_model.get_feature_names_out()\n",
    "    \n",
    "    # Get the row of the c-TF-IDF matrix corresponding to the topic\n",
    "    topic_row = c_tf_idf[topic]\n",
    "    \n",
    "    # Get the indices of the top_n words for the topic\n",
    "    top_n_indices = np.argsort(topic_row)[::-1][:top_n]\n",
    "    \n",
    "    # Get the words and their probabilities\n",
    "    words = [feature_names[i] for i in top_n_indices]\n",
    "    probabilities = [topic_row[i] for i in top_n_indices]\n",
    "    \n",
    "    # Return the words and their probabilities as a list of tuples\n",
    "    return list(zip(words, probabilities))\n",
    "\n",
    "def group_docs_by_topic(docs, bertopic_model):\n",
    "    \"\"\"\n",
    "    Group documents by their assigned topic.\n",
    "\n",
    "    This function takes as input a list of documents `docs` and a BERTopic model `bertopic_model`. It creates a dictionary where the keys are topic numbers and the values are lists of documents assigned to each topic.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        bertopic_model (BERTopic): The BERTopic model used to assign topics to the documents.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are topic numbers (int) and the values are lists of documents (list) assigned to each topic.\n",
    "    \"\"\"\n",
    "    docs_by_topic = {}\n",
    "    for doc, topic in zip(docs, bertopic_model.topics_):\n",
    "        if topic+1 not in docs_by_topic:\n",
    "            docs_by_topic[topic+1] = []\n",
    "        docs_by_topic[topic+1].append(doc)\n",
    "    \n",
    "    return docs_by_topic \n",
    "\n",
    "def get_word_freq(bertopic_model, docs, topic, top_n=10, scale=1, lemmatize=False):\n",
    "    \"\"\"\n",
    "    Get the word frequencies for a given topic.\n",
    "\n",
    "    This function takes as input a BERTopic model `bertopic_model`, a list of documents `docs`, a topic number `topic`, an optional integer parameter `top_n` specifying the number of words to include, an optional float parameter `scale` used to scale the probabilities of the words and an optional boolean parameter `lemmatize` which determines whether to lemmatize the words before calculating their frequencies.\n",
    "\n",
    "    The function first retrieves the top n words for the given topic using the `get_topic_words` function and scales their probabilities using the provided `scale` parameter. If `lemmatize` is `True`, the function lemmatizes the words using the `lemmatize_words` function and recalculates their probabilities using the `recalculate_probabilities` function. Otherwise, it uses the original words and their probabilities.\n",
    "\n",
    "    Parameters:\n",
    "        bertopic_model (BERTopic): The BERTopic model used to calculate the topic words.\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        topic (int): The topic number for which to calculate the word frequencies.\n",
    "        top_n (int): An optional integer parameter specifying the number of words to include. Defaults to 10.\n",
    "        scale (float): An optional float parameter used to scale the probabilities of the words. Defaults to 1.\n",
    "        lemmatize (bool): An optional boolean parameter used to determine whether to lemmatize the words before calculating their frequencies. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the words/lemmas (str) and the values are their probabilities (float).\n",
    "    \"\"\"\n",
    "    # Get the topic words and their probabilities\n",
    "    topic_words = get_topic_words(bertopic_model, topic, top_n=top_n)\n",
    "    # Scale the probabilities\n",
    "    topic_words = [(word, prob ** scale) for word, prob in topic_words]\n",
    "\n",
    "    if lemmatize:\n",
    "        # Group documents by their assigned topic.\n",
    "        docs_by_topic = group_docs_by_topic(docs, bertopic_model)\n",
    "        # get the documents assigned to a specific topic\n",
    "        my_docs = docs_by_topic.get(topic, [])\n",
    "        # Lemmatize the words and combine their probabilities\n",
    "        lemma_prob = lemmatize_words(topic_words)\n",
    "        # Recalculate the c-TF-IDF scores for the lemmas\n",
    "        topic_words_lemma = recalculate_probabilities(lemma_prob, my_docs, bertopic_model)\n",
    "        # Create a dictionary with the lemmas and their probabilities\n",
    "        word_freq = {lemma: prob for lemma, prob in topic_words_lemma.items()}\n",
    "    \n",
    "    else:\n",
    "        # Create a dictionary with the words and their probabilities\n",
    "        word_freq = {word: prob for word, prob in topic_words}\n",
    "    \n",
    "    return word_freq \n",
    "\n",
    "def create_wordcloud(bertopic_model, docs, topic, top_n=10, scale=1, lemmatize=False, stopwords=None, wordcloud_kwargs=None):\n",
    "    \"\"\"\n",
    "    Create a word cloud from a BERTopic model and a topic.\n",
    "\n",
    "    This function takes as input a BERTopic model `topic_model`, a list of documents `docs`, a topic number `topic`, an optional integer parameter `top_n` specifying the number of words to include in the word cloud, an optional float parameter `scale` used to scale the probabilities of the words, an optional boolean parameter `lemmatize` which determines whether to lemmatize the words before creating the word cloud, an optional list of stopwords `stopwords` to be removed from the word cloud, and an optional dictionary of keyword arguments `wordcloud_kwargs` to be passed to the WordCloud constructor.\n",
    "\n",
    "    The function first retrieves the top n words for the given topic using the `get_topic_words` function and scales their probabilities using the provided `scale` parameter. If `lemmatize` is `True`, the function lemmatizes the words using the `lemmatize_words` function and recalculates their probabilities using the `recalculate_probabilities` function. Otherwise, it uses the original words and their probabilities.\n",
    "\n",
    "    The function then removes any stopwords from the list of words (if provided) and creates a word cloud using the WordCloud class from the wordcloud library. The resulting word cloud is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        topic_model (BERTopic): The BERTopic model used to calculate the topic words.\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        topic (int): The topic number for which to create the word cloud.\n",
    "        top_n (int): An optional integer parameter specifying the number of words to include in the word cloud. Defaults to 10.\n",
    "        scale (float): An optional float parameter used to scale the probabilities of the words. Defaults to 1.\n",
    "        lemmatize (bool): An optional boolean parameter used to determine whether to lemmatize the words before creating the word cloud. Defaults to False.\n",
    "        stopwords (list): An optional list of stopwords to be removed from the word cloud. Defaults to None.\n",
    "        wordcloud_kwargs (dict): An optional dictionary of keyword arguments to be passed to the WordCloud constructor. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        wordcloud.WordCloud: The resulting word cloud.\n",
    "    \"\"\"\n",
    "    # Get the word frequencies for a given topic.\n",
    "    word_freq = get_word_freq(bertopic_model, docs, topic, top_n=top_n, scale=scale, lemmatize=lemmatize)\n",
    "\n",
    "    # Remove stopwords from word_freq if provided\n",
    "    if stopwords:\n",
    "        word_freq = {word: freq for word, freq in word_freq.items() if word not in stopwords}\n",
    "    \n",
    "    # Create the word cloud using the words/lemmas and their probabilities\n",
    "    wc = WordCloud(**(wordcloud_kwargs or {}))\n",
    "    wc.generate_from_frequencies(word_freq)\n",
    "\n",
    "    # Display the word cloud\n",
    "    # plt.imshow(wc, interpolation='bilinear')\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "    return wc\n",
    "\n",
    "def create_wordclouds_bertopic(bertopic_model, docs, top_n=10, scale=1, lemmatize=False, stopwords=None, wordcloud_kwargs=None, to_save=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Create word clouds for all topics in a BERTopic model.\n",
    "\n",
    "    This function takes as input a BERTopic model `bertopic_model`, a list of documents `docs`, an optional integer parameter `top_n` specifying the number of words to include in each word cloud, an optional float parameter `scale` used to scale the probabilities of the words, an optional boolean parameter `lemmatize` which determines whether to lemmatize the words before creating the word clouds, an optional list of stopwords `stopwords` to be removed from the word clouds, an optional dictionary of keyword arguments `wordcloud_kwargs` to be passed to the WordCloud constructor, an optional boolean parameter `to_save` which determines whether to save the word clouds as image files, and an optional string parameter `save_path` specifying the path where the image files should be saved.\n",
    "\n",
    "    The function first retrieves the topic information from the BERTopic model and sets the index of the resulting DataFrame to be the topic number. Then, it loops over the topic numbers in the DataFrame and calls the `create_wordcloud` function to create a word cloud for each topic. If `to_save` is `True`, it saves each word cloud as an image file at the specified location using the custom name of the topic.\n",
    "\n",
    "    The resulting dictionary, where the keys are the custom names of the topics and the values are their corresponding word clouds, is then returned.\n",
    "\n",
    "    Parameters:\n",
    "        bertopic_model (BERTopic): The BERTopic model used to calculate the topic words.\n",
    "        docs (list): A list of documents used to fit the BERTopic model.\n",
    "        top_n (int): An optional integer parameter specifying the number of words to include in each word cloud. Defaults to 10.\n",
    "        scale (float): An optional float parameter used to scale the probabilities of the words. Defaults to 1.\n",
    "        lemmatize (bool): An optional boolean parameter used to determine whether to lemmatize the words before creating the word clouds. Defaults to False.\n",
    "        stopwords (list): An optional list of stopwords to be removed from each word cloud. Defaults to None.\n",
    "        wordcloud_kwargs (dict): An optional dictionary of keyword arguments to be passed to each WordCloud constructor. Defaults to None.\n",
    "        to_save (bool): An optional boolean parameter used to determine whether to save each word cloud as an image file. Defaults to False.\n",
    "        save_path (str): An optional string parameter specifying the path where each image file should be saved. Only used if `to_save` is True. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the custom names of the topics (str) and the values are their corresponding word clouds (wordcloud.WordCloud).\n",
    "    \"\"\"\n",
    "    # Check that save_path is provided if to_save is True\n",
    "    if to_save and save_path is None:\n",
    "        raise ValueError(\"If to_save is True, save_path must be provided\")\n",
    "    \n",
    "    # Get the topic information\n",
    "    topic_info = bertopic_model.get_topic_info()\n",
    "    # Set the index of the DataFrame to be the topic number\n",
    "    topic_info = topic_info.set_index('Topic')\n",
    "    wc_pics={}\n",
    "    # Loop over the topic numbers in the DataFrame\n",
    "    for topic_number in topic_info.index:\n",
    "        # Get the custom name of the current topic for saving purpose\n",
    "        topic_custom_name = topic_info.loc[topic_number, 'CustomName']\n",
    "        wc_pic = create_wordcloud(bertopic_model, docs, topic_number+1, top_n=top_n, scale=scale, lemmatize=lemmatize, stopwords=stopwords, wordcloud_kwargs=wordcloud_kwargs)\n",
    "        wc_pics.update({topic_custom_name: wc_pic})\n",
    "\n",
    "        if to_save and lemmatize:\n",
    "            wc_pic.to_file(f'{save_path}/{topic_custom_name}_lemmatized.png')\n",
    "        elif to_save and lemmatize==False:\n",
    "            wc_pic.to_file(f'{save_path}/{topic_custom_name}.png')\n",
    "\n",
    "    return wc_pics\n",
    "\n",
    "# examples of default colors for the wordcloud\n",
    "# colormaps = [\n",
    "#     'viridis', 'plasma', 'inferno', 'magma', 'cividis',\n",
    "#     'Greys', 'Purples', 'Blues', 'Greens', 'Oranges',\n",
    "#     'Reds', 'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd',\n",
    "#     'RdPu', 'BuPu', 'GnBu', 'PuBu', 'YlGnBu',\n",
    "#     'PuBuGn', 'BuGn', 'YlGn'\n",
    "# ]\n",
    "\n",
    "# Create a custom colormap\n",
    "# colors = [\"#294D61\", \"#6DA5C0\", \"#0F969C\", \"#0C7075\", \"#072E33\",\"#05161A\"]\n",
    "colors = [\"#F1916D\", \"#F5D7DB\", \"#BD83B8\", \"#473E66\", \"#1B3358\",\"#06142E\"]\n",
    "my_cmap = ListedColormap(colors)\n",
    "\n",
    "# Create a binary mask from an image\n",
    "# make sure that the image is black and white, where black pixels indicate where to draw words and white pixels indicate where not to draw words.\n",
    "# mask = np.array(Image.open('circle.png'))\n",
    "\n",
    "wordcloud_stopwords = [\"get\",\"would\",\"us\",\"good\",\"like\",\"goods\",\"got\",\"able\",\"quite\",\"always\",\"nothing\",\"add\",\"everything\",\"think\",\"gave\",\"due\",\"find\",\"say\",\"took\",\"still\",\"within\",\"22\",\"10\",\"one\",\"makhloufi\",\"saliger\",\"kessler\",\"naldrin\",\"hidde\",\"mohua\",\"possible\",\"since\",\"could\",\"especially\",\"altivar\",\"every\",\"anyway\",\"egawa\",\"sometimes\"]\n",
    "\n",
    "wordcloud_kwargs = {\n",
    "    'width': 800,\n",
    "    'height': 400,\n",
    "    'background_color': 'black',\n",
    "    'contour_width': 100,\n",
    "    'contour_color': 'red',\n",
    "    'colormap': 'GnBu',\n",
    "    \"contour_width\": 5,\n",
    "    \"contour_color\": 'red'\n",
    "    # \"mask\":mask\n",
    "}\n",
    "\n",
    "wordclouds = create_wordclouds_bertopic(topic_model_merged, docs, \n",
    "                           top_n=50, scale=1, \n",
    "                           lemmatize=False, \n",
    "                           stopwords=wordcloud_stopwords, \n",
    "                           wordcloud_kwargs=wordcloud_kwargs,\n",
    "                           to_save=True, \n",
    "                           save_path=\"../data/wordclouds/new_topic_model_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wassati",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
